{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def Get_feature_df_list_for_all_ECG_type(data_folder, feature_folder):\n",
    "    st = time.time()\n",
    "    ECG_type_list = [\"SINUS\", \"JET\"]\n",
    "    channel_list = [\"ECG-Lead-I\", \"ECG-Lead-II\", \"ECG-Lead-III\", \"ECG-Lead-IV\"]\n",
    "    feature_df_list = []\n",
    "    for ECG_type in ECG_type_list:\n",
    "        print(f\"\\nLoading {ECG_type} data...\")\n",
    "        \"\"\" Read for patient info \"\"\"\n",
    "        download_report_path = os.path.join(data_folder, ECG_type, f\"download_report_{ECG_type}.csv\")\n",
    "        download_report_df = pd.read_csv(download_report_path)\n",
    "        counter = 0\n",
    "        for row in download_report_df.iterrows():\n",
    "            if counter % 60 == 0:\n",
    "                print(f\"{counter} / {len(download_report_df)}. Time: {time.time() - st:.4f}\", end=\", \")\n",
    "            counter += 1\n",
    "\n",
    "            interval_ID = row[1][\"Interval ID#\"]\n",
    "            patient_ID = row[1][\" reference\"]\n",
    "\n",
    "            \"\"\" Load ECG for each channel \"\"\"\n",
    "            for channel in channel_list:\n",
    "                feature_filename_channel = f\"patient-{patient_ID}_event-{interval_ID}_block-1_{channel}_{ECG_type}\"\n",
    "                feature_filepath = os.path.join(feature_folder, f\"{feature_filename_channel}.csv\")\n",
    "                if os.path.exists(feature_filepath):\n",
    "                    feature_df = pd.read_csv(feature_filepath)\n",
    "                    feature_df[\"r_t\"] = pd.to_datetime(feature_df[\"r_t\"], unit=\"s\")\n",
    "                    feature_df[\"r_ID_abs\"] = feature_df[\"start_ID\"] + feature_df[\"r_ID\"] - 1\n",
    "                    if len(feature_df) > 0:\n",
    "                        feature_df_list.append(feature_df)\n",
    "\n",
    "    return feature_df_list\n",
    "\n",
    "\n",
    "def Get_roc_curve_df_from_model_and_df(model, feature_df, target_fpr=0.1, target_threshold=None):\n",
    "    y_scores = model.predict_proba(feature_df[model.feature_names_in_])[:, 1]\n",
    "    y_true = feature_df[\"label\"].values\n",
    "    auroc = roc_auc_score(y_true, y_scores, average=\"weighted\")\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    roc_curve_df = pd.DataFrame([fpr, tpr, thresholds]).T\n",
    "    roc_curve_df.columns=[\"fpr\", \"tpr\", \"thresholds\"]\n",
    "    if target_threshold is not None:\n",
    "        closest_threshold_idx = np.argsort(np.abs(roc_curve_df[\"thresholds\"] - target_threshold))[0]\n",
    "    else:\n",
    "        closest_threshold_idx = np.argsort(np.abs(roc_curve_df[\"fpr\"] - target_fpr))[0]\n",
    "    selected_threshold = roc_curve_df[\"thresholds\"][closest_threshold_idx]\n",
    "    selected_fpr = roc_curve_df[\"fpr\"][closest_threshold_idx]\n",
    "    roc_curve_results_dict = {\"roc_curve_df\": roc_curve_df, \"selected_threshold\": selected_threshold, \"selected_fpr\": selected_fpr, \"auroc\": auroc}\n",
    "    return roc_curve_results_dict\n",
    "\n",
    "\n",
    "def Draw_roc_curve(model, df_train, df_test, target_fpr=0.1, suptitle=\"\"):\n",
    "    sns.set(font_scale=1.5)\n",
    "    roc_curve_results_dict_train = Get_roc_curve_df_from_model_and_df(model, df_train, target_fpr=target_fpr)\n",
    "    roc_curve_results_dict_test = Get_roc_curve_df_from_model_and_df(model, df_test, target_threshold=roc_curve_results_dict_train[\"selected_threshold\"])\n",
    "    roc_curve_results_dict_train[\"roc_curve_df\"][\"Dataset\"] = \"Training\"\n",
    "    roc_curve_results_dict_test[\"roc_curve_df\"][\"Dataset\"] = \"Test\"\n",
    "    roc_curve_df = pd.concat([roc_curve_results_dict_train[\"roc_curve_df\"], roc_curve_results_dict_test[\"roc_curve_df\"]], axis=0)\n",
    "\n",
    "    g = sns.relplot(data=roc_curve_df.query(\"thresholds <= 1\"), x=\"fpr\", y=\"tpr\", hue=\"thresholds\", col=\"Dataset\", height=6, kind=\"scatter\", palette=\"magma\",\n",
    "                    linewidth=0.01)\n",
    "    g.axes[0][0].axvline(roc_curve_results_dict_train[\"selected_fpr\"], linestyle=\":\")\n",
    "    g.axes[0][0].text(roc_curve_results_dict_train[\"selected_fpr\"], 0.3, f\"Selected threshold = {roc_curve_results_dict_train['selected_threshold']:.3f}\")\n",
    "    g.axes[0][0].set_title(f\"{g.axes[0][0].get_title()}\\nAUROC = {roc_curve_results_dict_train['auroc']:.4f}\")\n",
    "    g.axes[0][1].axvline(roc_curve_results_dict_test[\"selected_fpr\"], linestyle=\":\")\n",
    "    g.axes[0][1].text(roc_curve_results_dict_test[\"selected_fpr\"], 0.3, f\"Selected threshold = {roc_curve_results_dict_test['selected_threshold']:.3f}\")\n",
    "    g.axes[0][1].set_title(f\"{g.axes[0][1].get_title()}\\nAUROC = {roc_curve_results_dict_test['auroc']:.4f}\")\n",
    "    g.fig.suptitle(f\"{suptitle} - ROC curve\\n(target train FPR = {target_fpr})\")\n",
    "    plt.show()\n",
    "    os.makedirs(\"Figures\", exist_ok=True)\n",
    "    # plt.savefig(os.path.join(\"Figures\", \"ROC_curve.png\"), dpi=200)\n",
    "    return roc_curve_results_dict_train, roc_curve_results_dict_test\n",
    "\n",
    "\n",
    "\"\"\" A plot showing decision boundary \"\"\"\n",
    "\n",
    "\n",
    "def Get_contour_y_pred(model, feature_df, x1_var, x2_var, threshold=0.5):\n",
    "    contour_x_linspace = np.linspace(feature_df[x1_var].min(), feature_df[x1_var].max(), 301)\n",
    "    contour_y_linspace = np.linspace(feature_df[x2_var].min(), feature_df[x2_var].max(), 301)\n",
    "    coutour_xx, coutour_yy = np.meshgrid(contour_x_linspace, contour_y_linspace)\n",
    "    contour_df = pd.DataFrame(np.concatenate([coutour_xx.reshape(-1, 1), coutour_yy.reshape(-1, 1)], axis=1))\n",
    "    contour_df.columns = [x1_var, x2_var]\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        y_scores = model.predict_proba(contour_df)[:, 1]\n",
    "        y_pred = (y_scores > threshold).astype(int)\n",
    "    else:\n",
    "        y_scores = model.predict(contour_df)\n",
    "        y_pred = (y_scores > threshold).astype(int).values\n",
    "\n",
    "    contour_y_pred = y_pred.reshape(*coutour_xx.shape)\n",
    "    contour_result_dict = {\"contour_x_linspace\": contour_x_linspace, \"contour_y_linspace\": contour_y_linspace, \"contour_y_pred\": contour_y_pred}\n",
    "    return contour_result_dict\n",
    "\n",
    "\n",
    "def Draw_decision_boundary(model, df_train, df_test, colored_by_patient_ID=False, palette=\"bright\", target_fpr=0.1):\n",
    "    roc_curve_results_dict_train = Get_roc_curve_df_from_model_and_df(model, df_train, target_fpr=target_fpr)\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        x1_var = model.feature_names_in_[0]\n",
    "        x2_var = model.feature_names_in_[1]\n",
    "    else:\n",
    "        x1_var = list(dict(model.params).keys())[1]\n",
    "        x2_var = list(dict(model.params).keys())[2]\n",
    "    threshold_train = roc_curve_results_dict_train[\"selected_threshold\"]\n",
    "\n",
    "    \"\"\" Calculate predictions for contour \"\"\"\n",
    "    contour_result_dict_train = Get_contour_y_pred(model, df_train, x1_var, x2_var, threshold=threshold_train)\n",
    "\n",
    "    \"\"\" Calculate predictions for input data \"\"\"\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        y_pred_train = (model.predict_proba(df_train[model.feature_names_in_])[:, 1] > threshold_train).astype(int)\n",
    "        y_pred_test = (model.predict_proba(df_test[model.feature_names_in_])[:, 1] > threshold_train).astype(int)\n",
    "    else:\n",
    "        y_pred_train = (model.predict(df_train) > threshold_train).astype(int).values\n",
    "        y_pred_test = (model.predict(df_test) > threshold_train).astype(int).values\n",
    "\n",
    "    \"\"\" Plotting \"\"\"\n",
    "    sns.set(font_scale=2)\n",
    "    levels = mpl.ticker.MaxNLocator(nbins=2).tick_values(0, 1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(24, 8))\n",
    "\n",
    "    feature_df_renamed = df_train.rename(columns={\"label\": \"True label\"})\n",
    "    if colored_by_patient_ID:\n",
    "        feature_df_renamed[\"patient_ID\"] = feature_df_renamed[\"patient_ID\"].astype(int)\n",
    "        sns.scatterplot(data=feature_df_renamed, x=x1_var, y=x2_var, alpha=0.04, hue=\"patient_ID\", palette=palette, ax=axes[0])\n",
    "    else:\n",
    "        contour_handle = axes[0].contour(contour_result_dict_train[\"contour_x_linspace\"], contour_result_dict_train[\"contour_y_linspace\"], contour_result_dict_train[\"contour_y_pred\"], levels=levels, cmap=\"bwr\")\n",
    "        cbar = plt.colorbar(contour_handle, ax=axes[0])\n",
    "        cbar.ax.set_title(\"Prediction\")\n",
    "        sns.scatterplot(data=feature_df_renamed, x=x1_var, y=x2_var, alpha=0.01, hue=\"True label\", palette=[(0, 0, 0.8), (0.8, 0, 0)], ax=axes[0])\n",
    "    axes[0].set_title(\"Training data\")\n",
    "\n",
    "    feature_df_renamed = df_test.rename(columns={\"label\": \"True label\"})\n",
    "    if colored_by_patient_ID:\n",
    "        feature_df_renamed[\"patient_ID\"] = feature_df_renamed[\"patient_ID\"].astype(int)\n",
    "        sns.scatterplot(data=feature_df_renamed, x=x1_var, y=x2_var, alpha=0.04, hue=\"patient_ID\", palette=palette, ax=axes[1])\n",
    "    else:\n",
    "        contour_handle = axes[1].contour(contour_result_dict_train[\"contour_x_linspace\"], contour_result_dict_train[\"contour_y_linspace\"], contour_result_dict_train[\"contour_y_pred\"], levels=levels, cmap=\"bwr\")\n",
    "        cbar = plt.colorbar(contour_handle, ax=axes[1])\n",
    "        cbar.ax.set_title(\"Prediction\")\n",
    "        sns.scatterplot(data=feature_df_renamed, x=x1_var, y=x2_var, alpha=0.01, hue=\"True label\", palette=[(0, 0, 0.8), (0.8, 0, 0)], ax=axes[1])\n",
    "    axes[1].set_title(\"Test data\")\n",
    "\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        xlim = [-1, 4.5]\n",
    "        ylim = [-1, 9]\n",
    "    else:\n",
    "        xlim = [-0.005, 0.12]\n",
    "        ylim = [-0.02, 1.8]\n",
    "\n",
    "    axes[0].set_xlim(xlim)\n",
    "    axes[0].set_ylim(ylim)\n",
    "    axes[1].set_xlim(xlim)\n",
    "    axes[1].set_ylim(ylim)\n",
    "\n",
    "    plt.show()\n",
    "    os.makedirs(\"Figures\", exist_ok=True)\n",
    "    # plt.savefig(os.path.join(\"Figures\", \"decision_boundary.png\"), dpi=200)\n",
    "    return y_pred_train, y_pred_test, contour_result_dict_train\n",
    "\n",
    "def Get_confusion_matrix(model, data_df, threshold=0.5):\n",
    "    if isinstance(model, LogisticRegression):\n",
    "        y_pred = (model.predict_proba(data_df[model.feature_names_in_])[:, 1] > threshold).astype(int)\n",
    "    else:\n",
    "        y_pred = (model.predict(data_df) > threshold).astype(int).values\n",
    "    confusion_mat = np.histogram2d(data_df[\"label\"], y_pred, bins=2)[0]\n",
    "    return confusion_mat\n",
    "\n",
    "\n",
    "def Visualize_confusion_matrix(confusion_mat, ax=None, title=None):\n",
    "    if title is None:\n",
    "        title = \"Confusion matrix\"\n",
    "    confusion_name_mat = np.array([[\"TN\", \"FN\"], [\"FP\", \"TP\"]])\n",
    "\n",
    "    confusion_mat_percent = 100 * confusion_mat / np.sum(confusion_mat.ravel())\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.matshow(confusion_mat, cmap=\"summer\")\n",
    "    plt.colorbar()\n",
    "    for (i, j), v in np.ndenumerate(confusion_mat):\n",
    "        plt.text(i, j, f\"{v:.0f}\\n({confusion_mat_percent[i, j]:.0f}%, {confusion_name_mat[i, j]})\", ha=\"center\", va=\"center\", color=\"#000000\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.gca().grid(False)\n",
    "    plt.gca().set_xticklabels([\"\", \"SINUS\", \"JET\"])\n",
    "    plt.gca().set_yticklabels([\"\", \"SINUS\", \"JET\"])\n",
    "    plt.gcf().suptitle(title)\n",
    "\n",
    "    plt.show()\n",
    "    os.makedirs(\"Figures\", exist_ok=True)\n",
    "    # plt.savefig(os.path.join(\"Figures\", \"Confusion_matrix.png\"), dpi=200)\n",
    "\n",
    "\n",
    "def Get_download_report(data_folder):\n",
    "    download_report_JET = pd.read_csv(os.path.join(data_folder, \"JET\", \"download_report_JET.csv\"))\n",
    "    download_report_SINUS = pd.read_csv(os.path.join(data_folder, \"SINUS\", \"download_report_SINUS.csv\"))\n",
    "    download_report_JET = download_report_JET.rename(columns={\"Interval ID#\": \"interval_ID\", \" reference\": \"patient_ID\"})\n",
    "    download_report_SINUS = download_report_SINUS.rename(columns={\"Interval ID#\": \"interval_ID\", \" reference\": \"patient_ID\"})\n",
    "    download_report_JET[\"label\"] = 1\n",
    "    download_report_SINUS[\"label\"] = 0\n",
    "    download_report_all = pd.concat([download_report_JET, download_report_SINUS], axis=0).rename(columns={\"label\": \"True_label\"})\n",
    "    download_report_all[\"block_ID\"] = 1\n",
    "    download_report_all[\" start_time\"] = download_report_all[\" start_time\"].apply(Process_time_string)\n",
    "    download_report_all[\" stop_time\"] = download_report_all[\" stop_time\"].apply(Process_time_string)\n",
    "    print(download_report_all.columns)\n",
    "    return download_report_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer()\n",
    "\"\"\" Load data \"\"\"\n",
    "save_location = \"Data_20220201\"\n",
    "save_location = \"Data_20220105\"\n",
    "save_location = \"Data_20211201\"\n",
    "save_location = \"Data_20211109\"\n",
    "save_location = \"Data_20211028\"\n",
    "ecg_data_folder = \"JET-Detection\"\n",
    "download_report_all = Get_download_report(ecg_data_folder)\n",
    "\n",
    "feature_name_list = [\"pr_int_iqr\", \"p_prom_med\"]\n",
    "identifier_list = [\"cycle_ID\", \"patient_ID\", \"interval_ID\", \"block_ID\", \"channel_ID\", \"r_ID_abs\", \"label\", \"r_ID_abs_ref\"]\n",
    "\n",
    "feature_df_list_all = Get_feature_df_list_for_all_ECG_type(data_folder=ecg_data_folder, feature_folder=save_location)\n",
    "feature_df_all = pd.concat(feature_df_list_all, axis=0)\n",
    "\n",
    "common_heartbeats_df_selected = pd.read_csv(\"common_heartbeats_df_selected.csv\")\n",
    "feature_df_all_selected = feature_df_all.merge(common_heartbeats_df_selected, how=\"left\")\n",
    "feature_df_all_selected = feature_df_all_selected[~feature_df_all_selected[\"selected\"].isna()]\n",
    "timer.Print(\"Load data completed.\")\n",
    "\n",
    "\"\"\" Data preprocessing \"\"\"\n",
    "feature_df_all_nona_more_features_processed = feature_df_all_selected.copy()\n",
    "lead_II_and_not_aligned = feature_df_all_nona_more_features_processed[\"r_ID_abs_ref\"].isna() & (\n",
    "            feature_df_all_nona_more_features_processed[\"channel_ID\"] == 2)\n",
    "print(\"lead_II_and_not_aligned.sum()\", lead_II_and_not_aligned.sum())\n",
    "feature_df_all_nona_more_features_processed.loc[lead_II_and_not_aligned, \"r_ID_abs_ref\"] = feature_df_all_nona_more_features_processed.loc[\n",
    "    lead_II_and_not_aligned, \"r_ID_abs\"]\n",
    "\n",
    "feature_df_all_nona_more_features_processed_nona = feature_df_all_nona_more_features_processed[identifier_list + feature_name_list].dropna()\n",
    "feature_df_all_nona_more_features_processed_nona[\"r_ID_abs_ref\"] = feature_df_all_nona_more_features_processed_nona[\"r_ID_abs_ref\"].astype(int)\n",
    "\n",
    "feature_df_train = feature_df_all_nona_more_features_processed_nona.query(f\"patient_ID in {patient_ID_list_train}\")\n",
    "feature_df_test = feature_df_all_nona_more_features_processed_nona.query(f\"patient_ID in {patient_ID_list_test}\")\n",
    "feature_df_train_identifier = feature_df_train[identifier_list].reset_index(drop=True)\n",
    "feature_df_train_features = feature_df_train[feature_name_list].reset_index(drop=True)\n",
    "feature_df_test_identifier = feature_df_test[identifier_list].reset_index(drop=True)\n",
    "feature_df_test_features = feature_df_test[feature_name_list].reset_index(drop=True)\n",
    "print(f\"Train: {feature_df_train.shape}, Test: {feature_df_test.shape}, Total: {feature_df_all_nona_more_features_processed_nona.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(feature_df_train_features)\n",
    "feature_df_train_features_normalized = scaler.transform(feature_df_train_features)\n",
    "feature_df_train_features_normalized = pd.DataFrame(feature_df_train_features_normalized, columns=feature_name_list)\n",
    "feature_df_test_features_normalized = scaler.transform(feature_df_test_features)\n",
    "feature_df_test_features_normalized = pd.DataFrame(feature_df_test_features_normalized, columns=feature_name_list)\n",
    "feature_df_train_labels = feature_df_train_identifier[\"label\"]\n",
    "feature_df_test_labels = feature_df_test_identifier[\"label\"]\n",
    "\n",
    "feature_df_train_normalized = pd.concat([feature_df_train_identifier, feature_df_train_features_normalized], axis=1)\n",
    "feature_df_test_normalized = pd.concat([feature_df_test_identifier, feature_df_test_features_normalized], axis=1)\n",
    "timer.Print(\"Data preprocessing completed.\")\n",
    "\n",
    "\"\"\" Model fitting \"\"\"\n",
    "clf_dict = {}\n",
    "st = time.time()\n",
    "channel_ID_list = [2, 4]\n",
    "for channel_ID in channel_ID_list:\n",
    "    \"\"\" Preparing data \"\"\"\n",
    "    feature_df_train_normalized_lead = feature_df_train_normalized.query(f\"channel_ID == {channel_ID}\")\n",
    "    feature_df_test_normalized_lead = feature_df_test_normalized.query(f\"channel_ID == {channel_ID}\")\n",
    "    print(f\"[Time {time.time() - st:.1f}]\", channel_ID, feature_df_train_normalized_lead.shape, feature_df_test_normalized_lead.shape)\n",
    "\n",
    "    patient_ID_train = feature_df_train_normalized_lead[\"patient_ID\"]\n",
    "    X_train = feature_df_train_normalized_lead[feature_name_list]\n",
    "    y_train = feature_df_train_normalized_lead[\"label\"]\n",
    "    X_test = feature_df_test_normalized_lead[feature_name_list]\n",
    "    y_test = feature_df_test_normalized_lead[\"label\"]\n",
    "\n",
    "    \"\"\" Experiments setup \"\"\"\n",
    "    leave_one_group_out_cv = LeaveOneGroupOut()\n",
    "    tuned_parameters = [{'C': [1 / 1e-8, 1 / 0.03, 1 / 0.08, 1 / 0.3, 1 / 0.8, 1 / 3, 1 / 8, 1 / 30, 1 / 80, 1 / 300, 1 / 800, 1 / 3000, 1 / 8000, 1 / 30000, 1 / 80000, 1 / 300000, 1 / 800000]}] # sklearn uses 1 / C where C is the regularization strength\n",
    "\n",
    "    score = \"accuracy\"\n",
    "    print(f\"# Tuning hyper-parameters for {score}\\n\")\n",
    "    clf = GridSearchCV(\n",
    "        LogisticRegression(solver=\"saga\", random_state=0, penalty=\"l1\"), tuned_parameters, scoring=score, cv=leave_one_group_out_cv)\n",
    "\n",
    "    \"\"\" Run \"\"\"\n",
    "    clf.fit(X_train, y_train, groups=patient_ID_train.values)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\\n\")\n",
    "    print(f\"C = {1 / clf.best_params_['C']:.2f}\\n\")\n",
    "    print(\"Grid scores on development set (using 5-fold cross validation):\\n\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(f\"{mean:.4f} (+/-{std * 3:.4f}) for {1 / params['C']:.2f}\")\n",
    "\n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    y_score = clf.best_estimator_.predict_proba(X_test)\n",
    "    best_auroc = roc_auc_score(y_true, y_score[:, 1])\n",
    "    print(f\"Best roc_auc_score = {best_auroc:.4f}\\n\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    clf_dict[f\"lead{channel_ID}\"] = copy.deepcopy(clf)\n",
    "    timer.Print(f\"Channel {channel_ID} model fitted.\")\n",
    "\n",
    "    \"\"\" Visualize results \"\"\"\n",
    "    roc_curve_results_dict_train, roc_curve_results_dict_test = Draw_roc_curve(clf_dict[f\"lead{channel_ID}\"].best_estimator_, feature_df_train_normalized_lead[feature_name_list + [\"label\"]], feature_df_test_normalized_lead[feature_name_list + [\"label\"]], target_fpr=0.05)\n",
    "    confusion_mat_train = Get_confusion_matrix(clf_dict[f\"lead{channel_ID}\"].best_estimator_, feature_df_train_normalized_lead[feature_name_list + [\"label\"]], roc_curve_results_dict_train['selected_threshold'])\n",
    "    confusion_mat_test = Get_confusion_matrix(clf_dict[f\"lead{channel_ID}\"].best_estimator_, feature_df_test_normalized_lead[feature_name_list + [\"label\"]], roc_curve_results_dict_train['selected_threshold'])\n",
    "    Visualize_confusion_matrix(confusion_mat_train, title=\"Confusion matrix - train\")\n",
    "    Visualize_confusion_matrix(confusion_mat_test, title=\"Confusion matrix - test\")\n",
    "\n",
    "    y_pred_train, y_pred_test, contour_result_dict_train = Draw_decision_boundary(clf_dict[f\"lead{channel_ID}\"].best_estimator_, feature_df_train_normalized_lead[feature_name_list + [\"label\"]], feature_df_test_normalized_lead[feature_name_list + [\"label\"]], target_fpr=0.05)\n",
    "    print(f\"Lead {channel_ID} model coeffcients:\")\n",
    "    print(clf_dict[f\"lead{channel_ID}\"].best_estimator_.coef_, clf_dict[f\"lead{channel_ID}\"].best_estimator_.intercept_)\n",
    "    timer.Print(f\"Channel {channel_ID} model visualized.\")\n",
    "\n",
    "    \"\"\" Save error files for MATLAB \"\"\"\n",
    "    feature_df_test_with_prediction = feature_df_test.query(f\"channel_ID == {channel_ID}\").copy()\n",
    "    feature_df_test_with_prediction = feature_df_test_with_prediction.rename(columns={\"label\": \"True_label\"})\n",
    "    feature_df_test_with_prediction[\"Pred_label\"] = y_pred_test\n",
    "    feature_df_test_with_pred_incorrect = feature_df_test_with_prediction.query(\"True_label != Pred_label\")\n",
    "    feature_df_test_with_pred_incorrect_with_start_time = pd.merge(feature_df_test_with_pred_incorrect, download_report_all, on=[\"interval_ID\", \"patient_ID\", \"block_ID\", \"True_label\"], how=\"left\")\n",
    "\n",
    "    path_error = os.path.join(\"Errors\", f\"{save_location}_errors\")\n",
    "    os.makedirs(path_error, exist_ok=True)\n",
    "    np.random.seed(0)\n",
    "    feature_df_test_with_pred_incorrect_with_start_time.query(\"True_label == 1\").sample(frac=1).to_csv(\n",
    "        os.path.join(path_error, f\"feature_df_test_with_pred_incorrect_with_start_time_FN_Lead{channel_ID}.csv\"), index=False)\n",
    "    feature_df_test_with_pred_incorrect_with_start_time.query(\"True_label == 0\").sample(frac=1).to_csv(\n",
    "        os.path.join(path_error, f\"feature_df_test_with_pred_incorrect_with_start_time_FP_Lead{channel_ID}.csv\"), index=False)\n",
    "    timer.Print(f\"Channel {channel_ID} model error csv saved.\")\n",
    "\n",
    "    default_channel_ID = 2\n",
    "    if channel_ID == default_channel_ID:\n",
    "        feature_df_all_selected_with_ecg = pd.read_csv(os.path.join(\"feature_df_all_selected_with_ecg_20220210.csv\"))\n",
    "        feature_with_ecg_df_test = feature_df_all_selected_with_ecg.query(f\"patient_ID in {patient_ID_list_test}\")\n",
    "        feature_with_ecg_df_test_lead = feature_with_ecg_df_test.query(f\"channel_ID == {default_channel_ID}\").rename(columns={\"label\": \"True_label\"})\n",
    "        columns_to_drop = ['pr_int_iqr', 'p_prom_med']\n",
    "        feature_df_test_with_pred_incorrect_with_start_time_with_ecg = pd.merge(\n",
    "            feature_df_test_with_pred_incorrect_with_start_time.drop(columns=columns_to_drop), feature_with_ecg_df_test_lead,\n",
    "            on=[\"interval_ID\", \"patient_ID\", \"channel_ID\", \"r_ID_abs\", \"True_label\"], how=\"left\")\n",
    "        Plot_ecg_for_incorrect_predictions(feature_df_test_with_pred_incorrect_with_start_time_with_ecg, feature_df_all_selected_with_ecg, path_error=os.path.join(\"Errors\", f\"{save_location}_errors\"))\n",
    "        timer.Print(f\"Channel {channel_ID} model error ecg saved.\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import pickle5\n",
    "    os.makedirs(\"Models\", exist_ok=True)\n",
    "    with open(os.path.join(\"Models\", \"L1_logistic_regression.pickle\"), \"wb\") as f:\n",
    "        pickle5.dump(clf_dict, f)\n",
    "    timer.Print(f\"Channel {channel_ID} model saved.\")\n",
    "except:\n",
    "    print(\"Package pickle5 not installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T19:43:35.354181Z",
     "start_time": "2022-10-06T19:43:33.958867Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def Normalize(vec, eps=1e-8):\n",
    "    \"\"\" Normalize a 1d vector to 0-1 range \"\"\"\n",
    "    vec = vec - np.min(vec)\n",
    "    vec = vec / np.max(vec + eps)\n",
    "    return vec\n",
    "\n",
    "def Lower(word):\n",
    "    \"\"\" Convert word to lower case \"\"\"\n",
    "    return word.lower()\n",
    "\n",
    "class ECG_classification_dataset_with_peak_features(Dataset):\n",
    "    def __init__(self, feature_df_all_selected_p_ind_with_ecg, ecg_resampling_length_target=300,\n",
    "                 peak_loc_name=\"p_ind_resampled\", label_name=\"label\", short_identifier_list=None,\n",
    "                 peak_feature_name_list=None, shift_signal=False, shift_amount=None, normalize_signal=False,\n",
    "                 transforms=None, dataset_name=\"tch-ecg-jet-p40\"):\n",
    "        \"\"\"\n",
    "        normalize_signal: Normalize each individual signal to 0 - 1 range\n",
    "        \"\"\"\n",
    "        print(f\"ecg_resampling_length_target: {ecg_resampling_length_target}\")\n",
    "        if short_identifier_list is None:\n",
    "            short_identifier_list = ['patient_ID', 'interval_ID', 'block_ID', 'channel_ID', 'r_ID_abs', 'label',\n",
    "                                     'r_ID_abs_ref']\n",
    "        if peak_feature_name_list is None:\n",
    "            peak_feature_name_list = [\"p_prom_med\", \"pr_int_iqr\"]\n",
    "\n",
    "        if transforms is None:\n",
    "            self.transforms = []\n",
    "        else:\n",
    "            if isinstance(transforms, str):\n",
    "                transforms = [transforms]\n",
    "            self.transforms = [Lower(ele) for ele in transforms]\n",
    "\n",
    "        self.dataset_name = dataset_name\n",
    "        self.short_identifier_list = short_identifier_list\n",
    "        self.peak_feature_name_list = peak_feature_name_list\n",
    "        self.label_name = label_name\n",
    "        self.device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.feature_df_all_selected_p_ind_with_ecg = feature_df_all_selected_p_ind_with_ecg\n",
    "        self.ecg_resampling_length = 300\n",
    "        self.ecg_resampling_length_target = ecg_resampling_length_target\n",
    "        self.ecg_colnames = [f\"ecg{i + 1}\" for i in range(self.ecg_resampling_length)]\n",
    "        self.peak_loc_name = peak_loc_name\n",
    "        self.ecg_mat = self.feature_df_all_selected_p_ind_with_ecg[self.ecg_colnames].values\n",
    "        self.peak_label_list = self.feature_df_all_selected_p_ind_with_ecg[self.peak_loc_name].values\n",
    "        self.label_list = self.feature_df_all_selected_p_ind_with_ecg[self.label_name].values\n",
    "        self.short_identifier_mat = self.feature_df_all_selected_p_ind_with_ecg[self.short_identifier_list].values\n",
    "        self.peak_feature_mat = self.feature_df_all_selected_p_ind_with_ecg[self.peak_feature_name_list].values\n",
    "\n",
    "        self.shift_signal = shift_signal\n",
    "        self.shift_amount = shift_amount\n",
    "        self.normalize_signal = normalize_signal\n",
    "        if self.shift_signal:\n",
    "            if self.shift_amount is None:\n",
    "                self.shift_amount = 0\n",
    "            self.ecg_mat -= self.shift_amount  # Shift ECG to 0 baseline\n",
    "\n",
    "        if self.normalize_signal:\n",
    "            ecg_min = np.min(self.ecg_mat, axis=1)[:, np.newaxis]\n",
    "            ecg_max = np.max(self.ecg_mat, axis=1)[:, np.newaxis]\n",
    "            self.ecg_mat = (self.ecg_mat - ecg_min) / (ecg_max - ecg_min)\n",
    "\n",
    "    def obtain_perturbed_frame(self, frame):\n",
    "        # Adapted from https://github.com/danikiyasseh/CLOCS\n",
    "\n",
    "        \"\"\" Apply Sequence of Perturbations to Frame\n",
    "        Args:\n",
    "            frame (numpy array): frame containing ECG data\n",
    "        Outputs\n",
    "            frame (numpy array): perturbed frame based\n",
    "        \"\"\"\n",
    "        if Lower('Gaussian') in self.transforms:\n",
    "            mult_factor = 1\n",
    "            if self.dataset_name in ['ptb', 'physionet2020']:\n",
    "                # The ECG frames were normalized in amplitude between the values of 0 and 1.\n",
    "                variance_factor = 0.01 * mult_factor\n",
    "            elif self.dataset_name in ['cardiology', 'chapman']:\n",
    "                variance_factor = 10 * mult_factor\n",
    "            elif self.dataset_name in ['physionet', 'physionet2017']:\n",
    "                variance_factor = 100 * mult_factor\n",
    "            elif self.dataset_name in [\"tch-ecg-jet-p40\"]:\n",
    "                variance_factor = 0.01 * mult_factor\n",
    "            else:\n",
    "                raise NotImplementedError(\"Dataset not implemented\")\n",
    "            gauss_noise = np.random.normal(0, variance_factor, size=(self.ecg_resampling_length_target))\n",
    "            frame = frame + gauss_noise\n",
    "\n",
    "        if Lower('FlipAlongY') in self.transforms:\n",
    "            frame = np.flip(frame)\n",
    "\n",
    "        if Lower('FlipAlongX') in self.transforms:\n",
    "            frame = -frame\n",
    "\n",
    "        # Keep data in 0-1 range\n",
    "        frame = Normalize(frame)\n",
    "        return frame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_df_all_selected_p_ind_with_ecg)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.ecg_mat[idx, :]\n",
    "        if self.ecg_resampling_length_target != self.ecg_resampling_length:\n",
    "            X = resample_poly(X, int(self.ecg_resampling_length_target / 100), int(self.ecg_resampling_length / 100),\n",
    "                              padtype=\"line\")\n",
    "        X = Normalize(X)\n",
    "        X_aug = self.obtain_perturbed_frame(X)\n",
    "        peak_idx = self.peak_label_list[idx]\n",
    "        label = self.label_list[idx]\n",
    "        id_vec = self.short_identifier_mat[idx, :]\n",
    "        peak_features = self.peak_feature_mat[idx, :]\n",
    "\n",
    "        # return X[np.newaxis, :], peak_idx, label, id_vec, peak_features[np.newaxis, :]\n",
    "        if len(self.transforms) == 0:\n",
    "            return (X[np.newaxis, :], X_aug[np.newaxis, :]), label\n",
    "        else:\n",
    "            return X[np.newaxis, :], label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T19:57:39.465688Z",
     "start_time": "2022-10-06T19:57:38.705929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MacOS.\n",
      "Data shape: (10000, 371), train: (7642, 371), dev: (6690, 371), val: (952, 371), test: (2358, 371)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.signal import resample_poly\n",
    "\n",
    "patient_ID_list_train = [398573, 462229, 637891, 667681, 537854, 628521, 642321, 662493,\n",
    "                         387479, 624179, 417349, 551554, 631270, 655769, 678877]  # 15\n",
    "patient_ID_list_test = [756172, 424072, 748555, 748900, 759678, 741235, 595561, 678607,\n",
    "                        782501, 510915, 771495, 740475, 533362, 581650, 803389, 577874,\n",
    "                        681150, 536886, 477589, 844864, 824744, 515544, 771958, 725860, 609090]  # 25\n",
    "patient_ID_list_val = [462229, 642321, 387479]\n",
    "patient_ID_list_dev = [patient_ID for patient_ID in patient_ID_list_train if patient_ID not in patient_ID_list_val]\n",
    "\n",
    "\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    print(\"Using MacOS.\")\n",
    "    data_folder = os.path.normpath(\"/Users/yj31/Dropbox/Study/GitHub/JET-Detection\")\n",
    "    data_folder_2 = data_folder\n",
    "elif platform.system() == \"Linux\":\n",
    "    large_data_folder = data_folder_2 = data_folder = \"\"\n",
    "    print(\"Using Linux.\")\n",
    "else:\n",
    "    print(\"Using Windows.\")\n",
    "    data_folder = os.path.normpath(\"D:\\\\Dropbox\\\\Study\\\\GitHub\\\\JET-Detection\")\n",
    "    # data_folder_2 = os.path.normpath(\"D:\\\\Backup\\\\JET-Detection\\\\\")\n",
    "    data_folder_2 = data_folder\n",
    "    large_data_folder = os.path.normpath(\"D:\\\\Backup\\\\JET-Detection\\\\Heartbeats_dict_20220201\\\\\")\n",
    "\n",
    "save_folder = os.path.join(data_folder, \"Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Compute cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\mathbf{f}$: Time-course function in a particular pixel\n",
    "\n",
    "$f_i$: $i$th element (tick) of $\\mathbf{f}$\n",
    "\n",
    "$\\mu_f$: average of $\\mathbf{f}$\n",
    "\n",
    "$\\mathbf{r}$: Time-course function in a reference pixel - *mean of ECG of a given label type*\n",
    "\n",
    "$r_i$: $i$th element (tick) of $\\mathbf{r}$\n",
    "\n",
    "$\\mu_r$: average of $\\mathbf{r}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:00:06.949125Z",
     "start_time": "2022-10-06T20:00:06.854952Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecg1</th>\n",
       "      <th>ecg2</th>\n",
       "      <th>ecg3</th>\n",
       "      <th>ecg4</th>\n",
       "      <th>ecg5</th>\n",
       "      <th>ecg6</th>\n",
       "      <th>ecg7</th>\n",
       "      <th>ecg8</th>\n",
       "      <th>ecg9</th>\n",
       "      <th>ecg10</th>\n",
       "      <th>...</th>\n",
       "      <th>ecg291</th>\n",
       "      <th>ecg292</th>\n",
       "      <th>ecg293</th>\n",
       "      <th>ecg294</th>\n",
       "      <th>ecg295</th>\n",
       "      <th>ecg296</th>\n",
       "      <th>ecg297</th>\n",
       "      <th>ecg298</th>\n",
       "      <th>ecg299</th>\n",
       "      <th>ecg300</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.755326</td>\n",
       "      <td>-1.953039</td>\n",
       "      <td>-1.875564</td>\n",
       "      <td>-1.652560</td>\n",
       "      <td>-1.458367</td>\n",
       "      <td>-1.383229</td>\n",
       "      <td>-1.393706</td>\n",
       "      <td>-1.40426</td>\n",
       "      <td>-1.357915</td>\n",
       "      <td>-1.258599</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.016271</td>\n",
       "      <td>-1.173605</td>\n",
       "      <td>-1.304049</td>\n",
       "      <td>-1.398137</td>\n",
       "      <td>-1.470227</td>\n",
       "      <td>-1.550712</td>\n",
       "      <td>-1.663186</td>\n",
       "      <td>-1.790479</td>\n",
       "      <td>-1.854859</td>\n",
       "      <td>-1.754734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.516410</td>\n",
       "      <td>-0.586584</td>\n",
       "      <td>-0.650702</td>\n",
       "      <td>-0.602289</td>\n",
       "      <td>-0.492103</td>\n",
       "      <td>-0.451481</td>\n",
       "      <td>-0.494895</td>\n",
       "      <td>-0.53294</td>\n",
       "      <td>-0.521067</td>\n",
       "      <td>-0.488851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595169</td>\n",
       "      <td>-0.637709</td>\n",
       "      <td>-0.658493</td>\n",
       "      <td>-0.653338</td>\n",
       "      <td>-0.633022</td>\n",
       "      <td>-0.625785</td>\n",
       "      <td>-0.637344</td>\n",
       "      <td>-0.621625</td>\n",
       "      <td>-0.541468</td>\n",
       "      <td>-0.440227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ecg1      ecg2      ecg3      ecg4      ecg5      ecg6      ecg7  \\\n",
       "label                                                                         \n",
       "0     -1.755326 -1.953039 -1.875564 -1.652560 -1.458367 -1.383229 -1.393706   \n",
       "1     -0.516410 -0.586584 -0.650702 -0.602289 -0.492103 -0.451481 -0.494895   \n",
       "\n",
       "          ecg8      ecg9     ecg10  ...    ecg291    ecg292    ecg293  \\\n",
       "label                               ...                                 \n",
       "0     -1.40426 -1.357915 -1.258599  ... -1.016271 -1.173605 -1.304049   \n",
       "1     -0.53294 -0.521067 -0.488851  ... -0.595169 -0.637709 -0.658493   \n",
       "\n",
       "         ecg294    ecg295    ecg296    ecg297    ecg298    ecg299    ecg300  \n",
       "label                                                                        \n",
       "0     -1.398137 -1.470227 -1.550712 -1.663186 -1.790479 -1.854859 -1.754734  \n",
       "1     -0.653338 -0.633022 -0.625785 -0.637344 -0.621625 -0.541468 -0.440227  \n",
       "\n",
       "[2 rows x 300 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mean_summary = feature_with_ecg_df_train_single_lead.groupby([\"label\"]).agg({ele: \"mean\" for ele in ecg_colnames})\n",
    "class_mean_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:22:43.053324Z",
     "start_time": "2022-10-06T20:22:43.048251Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01881555115694953, 0.008923398589509102)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_mean_sinus = class_mean_summary.loc[0].values\n",
    "class_mean_jet = class_mean_summary.loc[1].values\n",
    "class_mean_sinus.shape, class_mean_jet.shape\n",
    "class_mean_sinus_mean = np.mean(class_mean_sinus)\n",
    "class_mean_jet_mean = np.mean(class_mean_jet)\n",
    "class_mean_sinus_mean, class_mean_jet_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:21:33.487205Z",
     "start_time": "2022-10-06T20:21:33.459691Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1230, 300), (420, 300))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg_mat_sinus = feature_with_ecg_df_train_single_lead.query(f\"label == 0\")[ecg_colnames].values\n",
    "ecg_mat_jet = feature_with_ecg_df_train_single_lead.query(f\"label == 1\")[ecg_colnames].values\n",
    "ecg_mat_sinus.shape, ecg_mat_jet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:32:54.050503Z",
     "start_time": "2022-10-06T20:32:54.041837Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$ \\displaystyle \\mathrm{Compute_cc}(X)\\triangleq <_ast.Assign object at 0x7fa7d16e8a90> $$"
      ],
      "text/plain": [
       "<latexify.core.with_latex.<locals>._LatexifiedFunction at 0x7fa7d16e8890>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import latexify\n",
    "@latexify.with_latex\n",
    "def Compute_cc(X):\n",
    "    ecg_mat_class = X\n",
    "#     ecg_mat_class = ecg_mat_sinus\n",
    "    ecg_class_mean = np.mean(ecg_mat_class, axis=0)\n",
    "    ecg_class_mean_mean = np.mean(ecg_class_mean)\n",
    "    ecg_class_mean_centered = (ecg_class_mean - ecg_class_mean_mean)[:, np.newaxis]\n",
    "\n",
    "    ecg_mat_class_centered = ecg_mat_class - np.mean(ecg_mat_class, axis=1)[:, np.newaxis]\n",
    "    ecg_mat_class_centered.shape, ecg_class_mean_centered.shape\n",
    "\n",
    "    cc = ecg_mat_class_centered @ ecg_class_mean_centered \\\n",
    "    / (np.linalg.norm(ecg_mat_class_centered, ord=2, axis=1) * np.linalg.norm(ecg_class_mean_centered, ord=2))\n",
    "\n",
    "    return cc\n",
    "Compute_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:26:17.150342Z",
     "start_time": "2022-10-06T20:26:17.143412Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1230, 300), (300, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecg_mat_class = ecg_mat_sinus\n",
    "ecg_class_mean = np.mean(ecg_mat_class, axis=0)\n",
    "ecg_class_mean_mean = np.mean(ecg_class_mean)\n",
    "ecg_class_mean_centered = (ecg_class_mean - ecg_class_mean_mean)[:, np.newaxis]\n",
    "\n",
    "ecg_mat_class_centered = ecg_mat_class - np.mean(ecg_mat_class, axis=1)[:, np.newaxis]\n",
    "ecg_mat_class_centered.shape, ecg_class_mean_centered.shape\n",
    "\n",
    "cc = ecg_mat_class_centered @ ecg_class_mean_centered \\\n",
    "    / (np.linalg.norm(ecg_mat_class_centered, ord=2, axis=1) * np.linalg.norm(ecg_class_mean_centered, ord=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:32:08.059571Z",
     "start_time": "2022-10-06T20:32:08.054678Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.16200177,  4.88625773,  7.96143958, ...,  4.21726409,\n",
       "       14.52191245, 14.38463665])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.sum(ecg_mat_class_centered ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:32:08.324402Z",
     "start_time": "2022-10-06T20:32:08.319655Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.16200177,  4.88625773,  7.96143958, ...,  4.21726409,\n",
       "       14.52191245, 14.38463665])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ecg_mat_class_centered, ord=2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T20:32:41.586251Z",
     "start_time": "2022-10-06T20:32:41.581283Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.024288781570759"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ecg_class_mean_centered, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:11:06.212561Z",
     "start_time": "2022-10-06T21:11:06.209305Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneGroupOut, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-06T21:38:26.237168Z",
     "start_time": "2022-10-06T21:36:05.307242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1899356, 370), train: (1447728, 370), dev: (1266878, 370), val: (180850, 370), test: (451628, 370)\n"
     ]
    }
   ],
   "source": [
    "# TODO: modify this step to make it use less memory at once\n",
    "debug = True\n",
    "debug = False\n",
    "# debug = args.debug\n",
    "if debug:\n",
    "    feature_df_all_selected_with_ecg = pd.read_csv(\n",
    "        os.path.join(data_folder_2, \"feature_df_all_selected_with_ecg_20220210_rtfixed_sample10000.csv\"))\n",
    "else:\n",
    "    feature_df_all_selected_with_ecg = pd.read_csv(\n",
    "        os.path.join(data_folder_2, \"feature_df_all_selected_with_ecg_20220210_rtfixed.csv\"))\n",
    "feature_with_ecg_df_train = feature_df_all_selected_with_ecg.query(f\"patient_ID in {patient_ID_list_train}\")\n",
    "feature_with_ecg_df_test = feature_df_all_selected_with_ecg.query(f\"patient_ID in {patient_ID_list_test}\")\n",
    "feature_with_ecg_df_dev = feature_df_all_selected_with_ecg.query(f\"patient_ID in {patient_ID_list_dev}\")\n",
    "feature_with_ecg_df_val = feature_df_all_selected_with_ecg.query(f\"patient_ID in {patient_ID_list_val}\")\n",
    "print(f\"Data shape: {feature_df_all_selected_with_ecg.shape}, \"\n",
    "      f\"train: {feature_with_ecg_df_train.shape}, \"\n",
    "      f\"dev: {feature_with_ecg_df_dev.shape}, \"\n",
    "      f\"val: {feature_with_ecg_df_val.shape}, \"\n",
    "      f\"test: {feature_with_ecg_df_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T00:17:12.639627Z",
     "start_time": "2022-10-06T21:38:28.378936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time 6.4] 2 (362268, 308) (114412, 308)\n",
      "(362268, 300) (362268,) (114412, 300) (114412,)\n",
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "C = 10000.00\n",
      "\n",
      "Grid scores on development set (using 5-fold cross validation):\n",
      "\n",
      "0.6615 (+/-0.8433) for 0.01\n",
      "0.6591 (+/-0.8452) for 0.03\n",
      "0.6590 (+/-0.8467) for 0.10\n",
      "0.6537 (+/-0.8462) for 0.32\n",
      "0.6556 (+/-0.8392) for 1.00\n",
      "0.6570 (+/-0.8458) for 3.16\n",
      "0.6511 (+/-0.8588) for 10.00\n",
      "0.6403 (+/-0.8715) for 31.60\n",
      "0.6324 (+/-0.8793) for 100.00\n",
      "0.6192 (+/-0.8625) for 316.00\n",
      "0.6199 (+/-0.8054) for 1000.00\n",
      "0.6402 (+/-0.7326) for 3160.00\n",
      "0.7069 (+/-0.7808) for 10000.00\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76     68589\n",
      "           1       0.98      0.08      0.14     45823\n",
      "\n",
      "    accuracy                           0.63    114412\n",
      "   macro avg       0.80      0.54      0.45    114412\n",
      "weighted avg       0.77      0.63      0.52    114412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_dict = {}\n",
    "st = time.time()\n",
    "channel_ID_list = [1, 2, 3, 4]\n",
    "channel_ID_list = [2]\n",
    "for channel_ID in channel_ID_list:\n",
    "#     channel_ID = 2\n",
    "    \"\"\" Get dataloader \"\"\"\n",
    "    feature_with_ecg_df_train_single_lead = feature_with_ecg_df_train.query(f\"channel_ID == {channel_ID}\")\n",
    "    feature_with_ecg_df_test_single_lead = feature_with_ecg_df_test.query(f\"channel_ID == {channel_ID}\")\n",
    "\n",
    "    ecg_resampling_length = 300\n",
    "    ecg_colnames = [f\"ecg{i + 1}\" for i in range(ecg_resampling_length)]\n",
    "    ecg_mat = feature_with_ecg_df_train_single_lead[ecg_colnames].values\n",
    "    signal_min_train = np.min(ecg_mat.ravel())\n",
    "    \n",
    "    feature_df_train = feature_with_ecg_df_train_single_lead\n",
    "    feature_df_test = feature_with_ecg_df_test_single_lead\n",
    "    identifier_list = [\"cycle_ID\", \"patient_ID\", \"interval_ID\", \"block_ID\", \"channel_ID\", \"r_ID_abs\", \"label\", \"r_ID_abs_ref\"]\n",
    "    feature_name_list = ecg_colnames\n",
    "    \n",
    "    \"\"\" Normalizing data \"\"\"\n",
    "    feature_df_train_identifier = feature_df_train[identifier_list].reset_index(drop=True)\n",
    "    feature_df_train_features = feature_df_train[feature_name_list].reset_index(drop=True)\n",
    "    feature_df_test_identifier = feature_df_test[identifier_list].reset_index(drop=True)\n",
    "    feature_df_test_features = feature_df_test[feature_name_list].reset_index(drop=True)\n",
    "    # print(f\"Train: {feature_df_train.shape}, Test: {feature_df_test.shape}, Total: {feature_df_all_nona_more_features_processed_nona.shape}\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(feature_df_train_features)\n",
    "    feature_df_train_features_normalized = scaler.transform(feature_df_train_features)\n",
    "    feature_df_train_features_normalized = pd.DataFrame(feature_df_train_features_normalized, columns=feature_name_list)\n",
    "    feature_df_test_features_normalized = scaler.transform(feature_df_test_features)\n",
    "    feature_df_test_features_normalized = pd.DataFrame(feature_df_test_features_normalized, columns=feature_name_list)\n",
    "    feature_df_train_labels = feature_df_train_identifier[\"label\"]\n",
    "    feature_df_test_labels = feature_df_test_identifier[\"label\"]\n",
    "    feature_df_train_normalized = pd.concat([feature_df_train_identifier, feature_df_train_features_normalized], axis=1)\n",
    "    feature_df_test_normalized = pd.concat([feature_df_test_identifier, feature_df_test_features_normalized], axis=1)\n",
    "    # timer.Print(\"Data preprocessing completed.\")\n",
    "\n",
    "    \"\"\" Preparing data \"\"\"\n",
    "    feature_df_train_normalized_lead = feature_df_train_normalized.query(f\"channel_ID == {channel_ID}\")\n",
    "    feature_df_test_normalized_lead = feature_df_test_normalized.query(f\"channel_ID == {channel_ID}\")\n",
    "    print(f\"[Time {time.time() - st:.1f}]\", channel_ID, feature_df_train_normalized_lead.shape, feature_df_test_normalized_lead.shape)\n",
    "\n",
    "    patient_ID_train = feature_df_train_normalized_lead[\"patient_ID\"]\n",
    "    X_train = feature_df_train_normalized_lead[feature_name_list]\n",
    "    y_train = feature_df_train_normalized_lead[\"label\"]\n",
    "    X_test = feature_df_test_normalized_lead[feature_name_list]\n",
    "    y_test = feature_df_test_normalized_lead[\"label\"]\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    \"\"\" Experiments setup \"\"\"\n",
    "    leave_one_group_out_cv = LeaveOneGroupOut()\n",
    "    tuned_parameters = [{\n",
    "        'C': [1 / 1e-6, 1 / 1e-4, 1 / 1e-2, 1 / 1, 1 / 1e2, 1 / 1e4, 1 / 1e6],\n",
    "        'C': [1 / 1e-2, 1 / 3.16e-2, 1 / 1e-1, 1 / 3.16e-1, 1 / 1, 1 / 3.16,\n",
    "              1 / 1e1, 1 / 3.16e1, 1 / 1e2, 1 / 3.16e2, 1 / 1e3, 1 / 3.16e3, 1 / 1e4],\n",
    "    }] # sklearn uses 1 / C where C is the regularization strength\n",
    "\n",
    "    score = \"accuracy\"\n",
    "    print(f\"# Tuning hyper-parameters for {score}\\n\")\n",
    "    clf = GridSearchCV(\n",
    "        LinearSVC(random_state=0, dual=False), tuned_parameters, scoring=score, cv=leave_one_group_out_cv)\n",
    "\n",
    "    \"\"\" Run \"\"\"\n",
    "    clf.fit(X_train, y_train, groups=patient_ID_train.values)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\\n\")\n",
    "    print(f\"C = {1 / clf.best_params_['C']:.2f}\\n\")\n",
    "    print(\"Grid scores on development set (using 5-fold cross validation):\\n\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(f\"{mean:.4f} (+/-{std * 3:.4f}) for {1 / params['C']:.2f}\")\n",
    "\n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    clf_dict[f\"lead{channel_ID}\"] = copy.deepcopy(clf)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-07T14:30:10.421499Z",
     "start_time": "2022-10-07T14:25:51.652223Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time 18.2] 2 (362268, 308) (114412, 308)\n",
      "(362268, 300) (362268,) (114412, 300) (114412,)\n",
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "C = 100000.00\n",
      "\n",
      "Grid scores on development set (using 5-fold cross validation):\n",
      "\n",
      "0.7115 (+/-0.7870) for 31600.00\n",
      "0.7122 (+/-0.7880) for 100000.00\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75     68589\n",
      "           1       0.48      0.00      0.00     45823\n",
      "\n",
      "    accuracy                           0.60    114412\n",
      "   macro avg       0.54      0.50      0.38    114412\n",
      "weighted avg       0.55      0.60      0.45    114412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# clf_dict = {}\n",
    "st = time.time()\n",
    "channel_ID_list = [1, 2, 3, 4]\n",
    "channel_ID_list = [2]\n",
    "for channel_ID in channel_ID_list:\n",
    "#     channel_ID = 2\n",
    "    \"\"\" Get dataloader \"\"\"\n",
    "    feature_with_ecg_df_train_single_lead = feature_with_ecg_df_train.query(f\"channel_ID == {channel_ID}\")\n",
    "    feature_with_ecg_df_test_single_lead = feature_with_ecg_df_test.query(f\"channel_ID == {channel_ID}\")\n",
    "\n",
    "    ecg_resampling_length = 300\n",
    "    ecg_colnames = [f\"ecg{i + 1}\" for i in range(ecg_resampling_length)]\n",
    "    ecg_mat = feature_with_ecg_df_train_single_lead[ecg_colnames].values\n",
    "    signal_min_train = np.min(ecg_mat.ravel())\n",
    "    \n",
    "    feature_df_train = feature_with_ecg_df_train_single_lead\n",
    "    feature_df_test = feature_with_ecg_df_test_single_lead\n",
    "    identifier_list = [\"cycle_ID\", \"patient_ID\", \"interval_ID\", \"block_ID\", \"channel_ID\", \"r_ID_abs\", \"label\", \"r_ID_abs_ref\"]\n",
    "    feature_name_list = ecg_colnames\n",
    "    \n",
    "    \"\"\" Normalizing data \"\"\"\n",
    "    feature_df_train_identifier = feature_df_train[identifier_list].reset_index(drop=True)\n",
    "    feature_df_train_features = feature_df_train[feature_name_list].reset_index(drop=True)\n",
    "    feature_df_test_identifier = feature_df_test[identifier_list].reset_index(drop=True)\n",
    "    feature_df_test_features = feature_df_test[feature_name_list].reset_index(drop=True)\n",
    "    # print(f\"Train: {feature_df_train.shape}, Test: {feature_df_test.shape}, Total: {feature_df_all_nona_more_features_processed_nona.shape}\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(feature_df_train_features)\n",
    "    feature_df_train_features_normalized = scaler.transform(feature_df_train_features)\n",
    "    feature_df_train_features_normalized = pd.DataFrame(feature_df_train_features_normalized, columns=feature_name_list)\n",
    "    feature_df_test_features_normalized = scaler.transform(feature_df_test_features)\n",
    "    feature_df_test_features_normalized = pd.DataFrame(feature_df_test_features_normalized, columns=feature_name_list)\n",
    "    feature_df_train_labels = feature_df_train_identifier[\"label\"]\n",
    "    feature_df_test_labels = feature_df_test_identifier[\"label\"]\n",
    "    feature_df_train_normalized = pd.concat([feature_df_train_identifier, feature_df_train_features_normalized], axis=1)\n",
    "    feature_df_test_normalized = pd.concat([feature_df_test_identifier, feature_df_test_features_normalized], axis=1)\n",
    "    # timer.Print(\"Data preprocessing completed.\")\n",
    "\n",
    "    \"\"\" Preparing data \"\"\"\n",
    "    feature_df_train_normalized_lead = feature_df_train_normalized.query(f\"channel_ID == {channel_ID}\")\n",
    "    feature_df_test_normalized_lead = feature_df_test_normalized.query(f\"channel_ID == {channel_ID}\")\n",
    "    print(f\"[Time {time.time() - st:.1f}]\", channel_ID, feature_df_train_normalized_lead.shape, feature_df_test_normalized_lead.shape)\n",
    "\n",
    "    patient_ID_train = feature_df_train_normalized_lead[\"patient_ID\"]\n",
    "    X_train = feature_df_train_normalized_lead[feature_name_list]\n",
    "    y_train = feature_df_train_normalized_lead[\"label\"]\n",
    "    X_test = feature_df_test_normalized_lead[feature_name_list]\n",
    "    y_test = feature_df_test_normalized_lead[\"label\"]\n",
    "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "    \"\"\" Experiments setup \"\"\"\n",
    "    leave_one_group_out_cv = LeaveOneGroupOut()\n",
    "    tuned_parameters = [{\n",
    "#         'C': [1 / 1e-6, 1 / 1e-4, 1 / 1e-2, 1 / 1, 1 / 1e2, 1 / 1e4, 1 / 1e6],\n",
    "#         'C': [1 / 1e-2, 1 / 3.16e-2, 1 / 1e-1, 1 / 3.16e-1, 1 / 1, 1 / 3.16,\n",
    "#               1 / 1e1, 1 / 3.16e1, 1 / 1e2, 1 / 3.16e2, 1 / 1e3, 1 / 3.16e3, 1 / 1e4],\n",
    "        'C': [1 / 3.16e4, 1 / 1e5],\n",
    "    }] # sklearn uses 1 / C where C is the regularization strength\n",
    "\n",
    "    score = \"accuracy\"\n",
    "    print(f\"# Tuning hyper-parameters for {score}\\n\")\n",
    "    clf = GridSearchCV(\n",
    "        LinearSVC(random_state=0, dual=False), tuned_parameters, scoring=score, cv=leave_one_group_out_cv)\n",
    "\n",
    "    \"\"\" Run \"\"\"\n",
    "    clf.fit(X_train, y_train, groups=patient_ID_train.values)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\\n\")\n",
    "    print(f\"C = {1 / clf.best_params_['C']:.2f}\\n\")\n",
    "    print(\"Grid scores on development set (using 5-fold cross validation):\\n\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(f\"{mean:.4f} (+/-{std * 3:.4f}) for {1 / params['C']:.2f}\")\n",
    "\n",
    "    print(\"\\nDetailed classification report:\\n\")\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    clf_dict[f\"lead{channel_ID}_largeC\"] = copy.deepcopy(clf)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
