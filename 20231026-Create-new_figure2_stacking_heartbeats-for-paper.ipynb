{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8818ae54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T03:41:12.152848200Z",
     "start_time": "2023-11-02T03:41:11.997169500Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.methods'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01monnx\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mstatsmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mformula\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msmf\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmethods\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msupervised_1d\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SupervisedModel_1D\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mCLOCS_1D\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cnn_network_contrastive\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mResNet1D\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ResNet1D\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'src.methods'"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from src.methods.supervised_1d import SupervisedModel_1D\n",
    "from src.models.CLOCS_1D import cnn_network_contrastive\n",
    "from src.models.ResNet1D import ResNet1D\n",
    "\n",
    "from src.utils.metrics import accuracy_at_k, weighted_mean, AUROC\n",
    "from src.utils.ECG_data_loading import *\n",
    "\n",
    "from lime_timeseries import LimeTimeSeriesExplainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a55f2",
   "metadata": {},
   "source": [
    "# Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698381e",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.device_count(), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01339cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:10:15.163498Z",
     "start_time": "2023-10-03T16:10:14.890528Z"
    }
   },
   "outputs": [],
   "source": [
    "softmax = torch.nn.Softmax(dim=1)\n",
    "def Get_dataset_id_dict(dataset):\n",
    "    dataset.return_id_vec = True\n",
    "    dataset_id_dict = {}\n",
    "    for i, (x, id_vec, y) in enumerate(dataset):\n",
    "        ECG_signal = x.ravel()\n",
    "        ECG_signal_str = \"_\".join([f\"{ele:.3f}\" for ele in ECG_signal])\n",
    "        id_dict = dict(zip(dataset.short_identifier_list, id_vec))\n",
    "        id_dict[\"label\"] = y\n",
    "        dataset_id_dict[ECG_signal_str] = id_dict\n",
    "\n",
    "    return dataset_id_dict\n",
    "\n",
    "\n",
    "def Lookup_ECG(ECG_signal, dataset_id_dict):\n",
    "    ECG_signal = ECG_signal.ravel()\n",
    "    ECG_signal_str = \"_\".join([f\"{ele:.3f}\" for ele in ECG_signal])\n",
    "    id_dict = dataset_id_dict.get(ECG_signal_str)\n",
    "    return id_dict\n",
    "\n",
    "\n",
    "def Get_roc_curve_df_from_model_and_data_loader(model, data_loader, target_fpr=0.1, target_threshold=None, use_gpu=False, abstain=False):\n",
    "    y_true_all_list = []\n",
    "    scores_all_list = []\n",
    "    abstention_all_list = []\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "    model = model.to(device)\n",
    "#     model.backbone = model.backbone.to(device)\n",
    "#     model.classifier = model.classifier.to(device)\n",
    "    model.eval()\n",
    "    auroc = AUROC(pos_label=1)\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        #         print(f\"Batch {i} / {len(data_loader)}\")\n",
    "        X, _, y = data\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        X_embedding = model.backbone(X).to(device)\n",
    "        X_logits = model.classifier(X_embedding).to(device)\n",
    "        y_true_all_list.append(y)\n",
    "\n",
    "        scores = softmax(X_logits)[:, 1].detach().cpu()\n",
    "        if abstain:\n",
    "            abstention_all_list.append(softmax(X_logits)[:, 2].detach().cpu())\n",
    "        scores_all_list.append(scores)\n",
    "        auroc.update(scores, y.detach().cpu())\n",
    "\n",
    "    auroc_value = auroc.compute()\n",
    "    auroc.reset()\n",
    "    print(f\"auroc_value = {auroc_value:.4f}\")\n",
    "    y_true_all = torch.cat(y_true_all_list, dim=0)\n",
    "    scores_all = torch.cat(scores_all_list, dim=0)\n",
    "    if abstain:\n",
    "        abstention_all = torch.cat(abstention_all_list, dim=0)\n",
    "    y_true_all_np = y_true_all.detach().cpu().numpy()\n",
    "    y_scores = scores_all\n",
    "    y_true = y_true_all_np\n",
    "\n",
    "    auroc = auroc_value\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    roc_curve_df = pd.DataFrame([fpr, tpr, thresholds]).T\n",
    "    roc_curve_df.columns = [\"fpr\", \"tpr\", \"thresholds\"]\n",
    "    if target_threshold is not None:\n",
    "        closest_threshold_idx = np.argsort(np.abs(roc_curve_df[\"thresholds\"] - target_threshold))[0]\n",
    "    else:\n",
    "        closest_threshold_idx = np.argsort(np.abs(roc_curve_df[\"fpr\"] - target_fpr))[0]\n",
    "    selected_threshold = roc_curve_df[\"thresholds\"][closest_threshold_idx]\n",
    "    selected_fpr = roc_curve_df[\"fpr\"][closest_threshold_idx]\n",
    "    roc_curve_results_dict = {\"roc_curve_df\": roc_curve_df, \"selected_threshold\": selected_threshold, \"selected_fpr\": selected_fpr, \"auroc\": auroc, \"y_scores\": y_scores, \"y_true\": y_true, \"abstention_all\": abstention_all}\n",
    "    return roc_curve_results_dict\n",
    "\n",
    "\n",
    "def Get_wrong_predictions_indices_from_roc_curve_results_dict(roc_curve_results_dict, target_threshold=None):\n",
    "    if target_threshold is None:\n",
    "        target_threshold = roc_curve_results_dict[\"selected_threshold\"]\n",
    "\n",
    "    y_true = roc_curve_results_dict[\"y_true\"]\n",
    "    if hasattr(roc_curve_results_dict[\"y_scores\"], \"numpy\"):\n",
    "        y_scores = roc_curve_results_dict[\"y_scores\"].numpy()\n",
    "    else:\n",
    "        y_scores = roc_curve_results_dict[\"y_scores\"]\n",
    "    y_pred = (y_scores > target_threshold).astype(int)\n",
    "    wrong_indices_dict = {0: np.where(~y_true & y_pred)[0].tolist(), 1: np.where(y_true & ~y_pred)[0].tolist()}\n",
    "    return wrong_indices_dict\n",
    "\n",
    "\n",
    "def Filter_data_by_id_dict(ecg_df, id_dict, num_cycle_to_show_each_side=4):\n",
    "    cycle_ID = ecg_df[\n",
    "        (ecg_df[\"patient_ID\"] == id_dict[\"patient_ID\"]) \\\n",
    "        & (ecg_df[\"interval_ID\"] == id_dict[\"interval_ID\"]) \\\n",
    "        & (ecg_df[\"block_ID\"] == id_dict[\"block_ID\"]) \\\n",
    "        & (ecg_df[\"channel_ID\"] == id_dict[\"channel_ID\"])\n",
    "        & (ecg_df[\"r_ID_abs\"] == id_dict[\"r_ID_abs\"])\n",
    "        ][\"cycle_ID\"].values[0]\n",
    "    ecg_df_selected = ecg_df[\n",
    "        (ecg_df[\"patient_ID\"] == id_dict[\"patient_ID\"]) \\\n",
    "        & (ecg_df[\"interval_ID\"] == id_dict[\"interval_ID\"]) \\\n",
    "        & (ecg_df[\"block_ID\"] == id_dict[\"block_ID\"]) \\\n",
    "        & (ecg_df[\"channel_ID\"] == id_dict[\"channel_ID\"]) \\\n",
    "        & ((ecg_df[\"cycle_ID\"] <= cycle_ID + num_cycle_to_show_each_side) \\\n",
    "           & (ecg_df[\"cycle_ID\"] >= cycle_ID - num_cycle_to_show_each_side))\n",
    "        ].sort_values(by=[\"cycle_ID\"], ascending=True)\n",
    "    return ecg_df_selected, cycle_ID\n",
    "\n",
    "\n",
    "def Show_LIME_explanation_for_idx_with_id(idx_list, dataset, NN_predict_proba, dataset_id_dict, dataset_df,\n",
    "                                          class_names=None, num_slices=30, num_cycle_to_show_each_side=4,\n",
    "                                          num_features=10, target_threshold=0.5,\n",
    "                                          replacement_method=\"total_mean\", entropy_list=None, text_to_show=None, uncertainty_name=None, uncertainty_value_list=None):\n",
    "    if class_names is None:\n",
    "        class_names = ['Sinus', 'JET']\n",
    "\n",
    "    nrow = len(idx_list)\n",
    "    ncol = 2 * num_cycle_to_show_each_side + 1\n",
    "    fig, axes = plt.subplots(nrow, ncol, figsize=(6 * ncol, 4 * nrow))\n",
    "    for i, idx in enumerate(idx_list):\n",
    "        ecg_signal, _, label = dataset[idx]\n",
    "        id_dict = Lookup_ECG(ecg_signal, dataset_id_dict)\n",
    "        ecg_df_selected, cycle_ID = Filter_data_by_id_dict(dataset_df, id_dict,\n",
    "                                                           num_cycle_to_show_each_side=num_cycle_to_show_each_side)\n",
    "        #         print(ecg_df_selected)\n",
    "        #         assert len(ecg_df_selected) == ncol\n",
    "        for j in range(len(ecg_df_selected)):\n",
    "            if nrow == 1:\n",
    "                ax = axes[j]\n",
    "            elif ncol == 1:\n",
    "                ax = axes[i]\n",
    "            else:\n",
    "                ax = axes[i, j]\n",
    "\n",
    "            cycle_ID_j = ecg_df_selected[\"cycle_ID\"].values[j]\n",
    "            patient_ID = ecg_df_selected[\"patient_ID\"].values[j]\n",
    "            ecg_signal = Normalize(ecg_df_selected[dataset.ecg_colnames].values[j, :])\n",
    "\n",
    "            uncertainty_value = uncertainty_value_list[idx] if uncertainty_value_list is not None else -1\n",
    "            uncertainty_text = f\"{uncertainty_name}\" if uncertainty_value_list is not None else \"\"\n",
    "            uncertainty_text_short = f\"{uncertainty_value:.3f}\" if uncertainty_value_list is not None else \"\"\n",
    "            if j == len(ecg_df_selected) // 2:\n",
    "                entropy_text = f\", Ent = {entropy_list[i]:.3f}\" if entropy_list is not None else \"\"\n",
    "                text_to_show_text = f\", {text_to_show}\" if text_to_show is not None else \"\"\n",
    "                ax.set_title(f\"Patient ID: {patient_ID}{text_to_show_text}{entropy_text}{uncertainty_text_short}\")\n",
    "            elif j == 0:\n",
    "                ax.set_title(f\"{uncertainty_text}\")\n",
    "            else:\n",
    "                pass\n",
    "#                 ax.set_title(f\"{uncertainty_text_short}\")\n",
    "\n",
    "            ax.set_ylabel(f\"Cycle {cycle_ID_j}\")\n",
    "\n",
    "            explainer = LimeTimeSeriesExplainer(class_names=class_names)\n",
    "#             print(f\"ecg_signal.shape: {ecg_signal.shape}\")\n",
    "            exp = explainer.explain_instance(\n",
    "                ecg_signal, NN_predict_proba, num_features=num_features,\n",
    "                num_samples=5000, num_slices=num_slices,\n",
    "                replacement_method=replacement_method)\n",
    "\n",
    "            Plot_LIME_explanation(ecg_signal, label, NN_predict_proba, exp, target_threshold=target_threshold,\n",
    "                                  num_slices=num_slices, num_features=num_features, ax=ax)\n",
    "\n",
    "\n",
    "#             return id_dict\n",
    "\n",
    "def Plot_LIME_explanation(ecg_signal, label, NN_predict_proba, exp,\n",
    "                          num_slices=30, num_features=10, ax=None, target_threshold=0.5):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    sns.set(style=\"white\", font_scale=1.5)\n",
    "    proba = NN_predict_proba(ecg_signal[np.newaxis, :]).ravel()\n",
    "    values_per_slice = np.ceil(len(ecg_signal) / num_slices).astype(int)\n",
    "    label_dict = {0: \"Sinus\", 1: \"JET\"}\n",
    "    color_dict = {0: cm.tab10(0), 1: cm.tab10(1)}\n",
    "    for i, p in enumerate(proba):\n",
    "        ax.text(20, 0.9 - 0.1 * i,\n",
    "                f\"Pr(y={label_dict[i]}|x; CNN) = {p:.2f}\", size=15)\n",
    "\n",
    "    if proba[1] < target_threshold and label == 1:\n",
    "        error = \"FN\"\n",
    "        error_color = f\"#FF00FF\"\n",
    "    elif proba[1] > target_threshold and label == 0:\n",
    "        error = \"FP\"\n",
    "        error_color = f\"#FF0000\"\n",
    "    else:\n",
    "        error = None\n",
    "        error_color = None\n",
    "\n",
    "    ax.plot(ecg_signal, color=color_dict[label], label=f\"True label: {label_dict[label]}\")\n",
    "    if error is not None:\n",
    "        ax.text(0.05 * len(ecg_signal), 0.1, error, color=error_color)\n",
    "    ax.legend(loc='lower center')\n",
    "\n",
    "    slice_intervals = np.arange(0, len(ecg_signal) + 1, values_per_slice)\n",
    "    slice_interval_midpoint_list = [(slice_intervals[i] + slice_intervals[i + 1]) / 2 \\\n",
    "                                    for i in range(len(slice_intervals) - 1)]\n",
    "    interval_val_tuple_list = []\n",
    "    for i in range(num_features):\n",
    "        feature, weight = exp.as_list()[i]\n",
    "        start = feature * values_per_slice\n",
    "        end = start + values_per_slice\n",
    "        color = 'red' if weight < 0 else 'green'\n",
    "        ax.axvspan(start, end, color=color, alpha=abs(weight * 2))\n",
    "        #         print(i, start, end)\n",
    "        #         ax.text(start, 0.1, f\"{weight:.2f}\") # This is the coefficient of the feature (slice)\n",
    "        # learned by the underlying linear model\n",
    "        interval_val_tuple_list.append((start, end, weight))\n",
    "    ax_twinx = ax.twinx()\n",
    "\n",
    "    interval_val_list = []\n",
    "    for slice_interval_midpoint in slice_interval_midpoint_list:\n",
    "        for j, interval_val_tuple in enumerate(interval_val_tuple_list):\n",
    "            if slice_interval_midpoint > interval_val_tuple[0] and slice_interval_midpoint < interval_val_tuple[1]:\n",
    "                interval_val_list.append(interval_val_tuple[2])\n",
    "                break\n",
    "        else:\n",
    "            interval_val_list.append(0)\n",
    "\n",
    "    #     print(len(slice_interval_midpoint_list), slice_interval_midpoint_list)\n",
    "    #     print(len(interval_val_list), interval_val_list)\n",
    "    ax_twinx.bar(x=slice_interval_midpoint_list, height=interval_val_list, color=\"#000000\", width=0.5 * values_per_slice)\n",
    "    ax_twinx.set_ylim(np.minimum(0, 2 * np.min(interval_val_list)),\n",
    "                      np.maximum(0, 2 * np.max(interval_val_list)))\n",
    "    ax_twinx.set_ylim(-0.25, 0.25)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax_twinx.set_yticklabels([])\n",
    "\n",
    "\n",
    "def Get_NN_predict_proba(model, use_gpu=False):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "    model.eval().to(device)\n",
    "\n",
    "    def NN_predict_proba(X):\n",
    "        \n",
    "        X = torch.from_numpy(X).to(device)\n",
    "        X = X.unsqueeze(1)\n",
    "        X_embedding = model.backbone(X).to(device)\n",
    "        X_logits = model.classifier(X_embedding).to(device)\n",
    "        predict_proba = softmax(X_logits).detach().cpu().numpy()\n",
    "        return predict_proba\n",
    "\n",
    "    return NN_predict_proba\n",
    "\n",
    "\n",
    "def Get_Entropy(p):\n",
    "    return (-p * np.log(p)).sum(axis=1)\n",
    "\n",
    "def Get_y_scores_and_entropy(roc_curve_results_dict_dict_dict, model_list, model_coef_list=None, mode=\"val\"):\n",
    "    y_true = roc_curve_results_dict_dict_dict[mode][model_list[0]][\"y_true\"]\n",
    "    y_score_list_weighted = []\n",
    "    y_score_list_unweighted = []\n",
    "    y_score_mat_list = []\n",
    "    \n",
    "    if model_coef_list is None:\n",
    "        model_coef_list = np.ones(shape=len(model_list)) / len(model_list)\n",
    "    for model_key, model_coef in zip(model_list, model_coef_list):\n",
    "#         print(model_coef, model_key)\n",
    "        y_scores_tmp = roc_curve_results_dict_dict_dict[mode][model_key][\"y_scores\"].cpu().numpy()[:, np.newaxis]\n",
    "        y_score_list_weighted.append(model_coef * y_scores_tmp)\n",
    "        y_score_mat_list.append(np.concatenate([(1 - y_scores_tmp), y_scores_tmp], axis=1))\n",
    "        y_score_list_unweighted.append(y_scores_tmp)\n",
    "\n",
    "    \"\"\" entropy \"\"\"\n",
    "    y_score_weighted = np.concatenate(y_score_list_weighted, axis=1).sum(axis=1)\n",
    "    y_scores_mat = np.concatenate([(1 - y_score_weighted)[:, np.newaxis], y_score_weighted[:, np.newaxis]], axis=1)\n",
    "    entropy = Get_Entropy(y_scores_mat)\n",
    "#     print(y_scores_mat[np.where(np.isnan(entropy))[0]])\n",
    "    \n",
    "    \"\"\" variance \"\"\"\n",
    "    y_score_unweighted = np.concatenate(y_score_list_unweighted, axis=1)\n",
    "    y_score_unweighted_variance = np.var(y_score_unweighted, axis=1)\n",
    "    y_score_unweighted_variance_0 = np.var(1 - y_score_unweighted, axis=1)\n",
    "    variance_prob = y_score_unweighted_variance + y_score_unweighted_variance_0\n",
    "    \n",
    "    \"\"\" JSD \"\"\"\n",
    "    entropy_list = [Get_Entropy(ele)[:, np.newaxis] for ele in y_score_mat_list]\n",
    "    entropy_mat = np.concatenate(entropy_list, axis=1)\n",
    "    entropy_mean = np.mean(entropy_mat, axis=1)\n",
    "    JSD = entropy - entropy_mean\n",
    "    \n",
    "    \"\"\" Softmax response (SR) \"\"\"\n",
    "    softmax_response = np.max(y_scores_mat, axis=1)\n",
    "    \n",
    "    uncertainty_dict = {\n",
    "        \"entropy\": entropy,\n",
    "        \"variance_prob\": variance_prob,\n",
    "        \"JSD\": JSD,\n",
    "        \"softmax_response\": softmax_response\n",
    "    }\n",
    "    \n",
    "    return y_score_weighted, uncertainty_dict\n",
    "\n",
    "def Get_y_scores_and_entropy_abstain(roc_curve_results_dict_dict_dict, model_list, model_coef_list=None, mode=\"val\"):\n",
    "    y_true = roc_curve_results_dict_dict_dict[mode][model_list[0]][\"y_true\"]\n",
    "    y_score_list_weighted = []\n",
    "    y_score_list_unweighted = []\n",
    "    y_score_mat_list = []\n",
    "    abstention_score_list = []\n",
    "    \n",
    "    if model_coef_list is None:\n",
    "        model_coef_list = np.ones(shape=len(model_list)) / len(model_list)\n",
    "    for model_key, model_coef in zip(model_list, model_coef_list):\n",
    "#         print(model_coef, model_key)\n",
    "        abstention_score_tmp = roc_curve_results_dict_dict_dict[mode][model_key][\"abstention_all\"].cpu().numpy()[:, np.newaxis]\n",
    "        y_scores_raw_tmp = roc_curve_results_dict_dict_dict[mode][model_key][\"y_scores\"].cpu().numpy()[:, np.newaxis]\n",
    "        prob_class_0 = 1 - np.sum(np.concatenate([y_scores_raw_tmp, abstention_score_tmp], axis=1), axis=1, keepdims=True)\n",
    "        prob_class_01 = np.concatenate([prob_class_0, y_scores_raw_tmp], axis=1)\n",
    "        prob_class_01_norm = prob_class_01 / np.sum(prob_class_01, axis=1, keepdims=True)\n",
    "#         print(f\"prob_class_0:\\n{prob_class_0}\")\n",
    "#         print(f\"prob_class_01:\\n{prob_class_01}\")\n",
    "#         print(f\"prob_class_01_norm:\\n{prob_class_01_norm}\")\n",
    "        y_scores_tmp = prob_class_01_norm[:, 1][:, np.newaxis]\n",
    "#         print(f\"y_scores_tmp:\\n{y_scores_tmp}\")\n",
    "        \n",
    "        y_score_list_weighted.append(model_coef * y_scores_tmp)\n",
    "        y_score_mat_list.append(np.concatenate([(1 - y_scores_tmp), y_scores_tmp], axis=1))\n",
    "        y_score_list_unweighted.append(y_scores_tmp)\n",
    "        abstention_score_list.append(model_coef * abstention_score_tmp)\n",
    "\n",
    "    abstention_score_mat = np.concatenate(abstention_score_list, axis=1)\n",
    "    abstention_score_weighted = np.sum(abstention_score_mat, axis=1)\n",
    "    y_score_weighted_vec = np.sum(y_score_list_weighted, axis=1)\n",
    "        \n",
    "    \"\"\" entropy \"\"\"\n",
    "    y_score_weighted = np.concatenate(y_score_list_weighted, axis=1).sum(axis=1)\n",
    "    y_scores_mat = np.concatenate([(1 - y_score_weighted)[:, np.newaxis], y_score_weighted[:, np.newaxis]], axis=1)\n",
    "    entropy = Get_Entropy(y_scores_mat)\n",
    "#     print(y_scores_mat[np.where(np.isnan(entropy))[0]])\n",
    "    \n",
    "    \"\"\" variance \"\"\"\n",
    "    y_score_unweighted = np.concatenate(y_score_list_unweighted, axis=1)\n",
    "    y_score_unweighted_variance = np.var(y_score_unweighted, axis=1)\n",
    "    y_score_unweighted_variance_0 = np.var(1 - y_score_unweighted, axis=1)\n",
    "    variance_prob = y_score_unweighted_variance + y_score_unweighted_variance_0\n",
    "    \n",
    "    \"\"\" JSD \"\"\"\n",
    "    entropy_list = [Get_Entropy(ele)[:, np.newaxis] for ele in y_score_mat_list]\n",
    "    entropy_mat = np.concatenate(entropy_list, axis=1)\n",
    "    entropy_mean = np.mean(entropy_mat, axis=1)\n",
    "    JSD = entropy - entropy_mean\n",
    "    \n",
    "    \"\"\" Softmax response (SR) \"\"\"\n",
    "    softmax_response = np.max(y_scores_mat, axis=1)\n",
    "    \n",
    "    uncertainty_dict = {\n",
    "        \"entropy\": entropy,\n",
    "        \"variance_prob\": variance_prob,\n",
    "        \"JSD\": JSD,\n",
    "        \"softmax_response\": softmax_response,\n",
    "        \"abstention_score\": abstention_score_weighted,\n",
    "        \"y_score_weighted_vec\": y_score_weighted_vec\n",
    "    }\n",
    "    \n",
    "    return y_score_weighted, uncertainty_dict\n",
    "\n",
    "\n",
    "def Get_NN_prob_for_emsemble(model_dict, model_coef_dict, use_gpu=False):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_gpu else \"cpu\")\n",
    "\n",
    "    def NN_predict_proba(X):\n",
    "        predict_proba_ens = None\n",
    "        for model_key, coef in model_coef_dict.items():\n",
    "            model = model_dict[model_key]\n",
    "            model.eval().to(device)\n",
    "            if isinstance(X, np.ndarray):\n",
    "                X = torch.from_numpy(X)\n",
    "                X = X.unsqueeze(1)\n",
    "            X = X.to(device)\n",
    "            X_embedding = model.backbone(X).to(device)\n",
    "            X_logits = model.classifier(X_embedding).to(device)\n",
    "            predict_proba = softmax(X_logits).detach().cpu().numpy()\n",
    "            if predict_proba_ens is None:\n",
    "                predict_proba_ens = coef * predict_proba\n",
    "            else:\n",
    "                predict_proba_ens += coef * predict_proba\n",
    "            \n",
    "        return predict_proba_ens\n",
    "    return NN_predict_proba\n",
    "\n",
    "def Get_roc_curve_df_for_ensemble_from_roc_curve_results_dict(roc_curve_results_dict_dict_dict, mode, model_coef_dict, target_fpr=0.1, target_threshold=None):\n",
    "    \n",
    "    model_list = list(model_coef_dict.keys())\n",
    "    model_coef_list = list(model_coef_dict.values())\n",
    "    y_scores, uncertainty_dict = Get_y_scores_and_entropy(roc_curve_results_dict_dict_dict, model_list, model_coef_list=model_coef_list, mode=mode)\n",
    "    y_true = roc_curve_results_dict_dict_dict[mode][model_list[0]][\"y_true\"]\n",
    "\n",
    "    auroc = AUROC(pos_label=1)\n",
    "    auroc.update(torch.from_numpy(y_scores), torch.from_numpy(y_true))\n",
    "    auroc_value = auroc.compute()\n",
    "    auroc.reset()\n",
    "    auroc = auroc_value\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    roc_curve_df = pd.DataFrame([fpr, tpr, thresholds]).T\n",
    "    roc_curve_df.columns = [\"fpr\", \"tpr\", \"thresholds\"]\n",
    "    if target_threshold is not None:\n",
    "        closest_threshold_idx = np.argsort(np.abs(roc_curve_df[\"thresholds\"] - target_threshold))[0]\n",
    "    else:\n",
    "        closest_threshold_idx = np.argsort(np.abs(roc_curve_df[\"fpr\"] - target_fpr))[0]\n",
    "    selected_threshold = roc_curve_df[\"thresholds\"][closest_threshold_idx]\n",
    "    selected_fpr = roc_curve_df[\"fpr\"][closest_threshold_idx]\n",
    "    roc_curve_results_dict = {\"roc_curve_df\": roc_curve_df, \"selected_threshold\": selected_threshold, \"selected_fpr\": selected_fpr,\n",
    "                              \"auroc\": auroc, \"y_scores\": y_scores, \"y_true\": y_true}\n",
    "    roc_curve_results_dict.update(uncertainty_dict)\n",
    "    return roc_curve_results_dict\n",
    "\n",
    "def Get_performance_with_abstention(uncertainty_value_list, threshold, y_scores, y_true, smaller_better=True, target_threshold=None, target_fpr=0.05):\n",
    "    \"\"\"\n",
    "    uncertainty_value_list: unsorted uncertainty values ordered the same as the raw data\n",
    "    \"\"\"\n",
    "    selected_idx_list = np.where(uncertainty_value_list < threshold)[0] if smaller_better else np.where(uncertainty_value_list > threshold)[0]\n",
    "    if len(selected_idx_list) == 0:\n",
    "        return None, None, None, None\n",
    "    proportion_abstained = (len(y_true) - len(selected_idx_list)) / len(y_true)\n",
    "    y_scores = y_scores[selected_idx_list]\n",
    "    y_true = y_true[selected_idx_list]\n",
    "    \n",
    "    \n",
    "    auroc = AUROC(pos_label=1)\n",
    "    auroc.update(torch.from_numpy(y_scores), torch.from_numpy(y_true))\n",
    "    auroc_value = auroc.compute()\n",
    "    auroc.reset()\n",
    "    auroc = auroc_value.item()\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores, pos_label=1)\n",
    "    roc_curve_df = pd.DataFrame([fpr, tpr, thresholds]).T\n",
    "    roc_curve_df.columns = [\"fpr\", \"tpr\", \"thresholds\"]\n",
    "    roc_curve_df_target_fpr = roc_curve_df.iloc[(roc_curve_df[\"fpr\"] - target_fpr).abs().argsort()[0]]\n",
    "    resulting_tpr = roc_curve_df_target_fpr[\"tpr\"]\n",
    "    prediction_cutoff = roc_curve_df_target_fpr[\"thresholds\"]\n",
    "    return auroc, resulting_tpr, proportion_abstained, prediction_cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391474f",
   "metadata": {},
   "source": [
    "# Load ECG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae3a8044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:10:58.695499Z",
     "start_time": "2023-10-03T16:10:16.500499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Loaded] val data: (44723, 370)\n",
      "[Dataset Loaded] test data: (114412, 370)\n",
      "[Dataset Created] val data: 44723\n",
      "[Dataset Created] test data: 114412\n",
      "[Dataset ID Dict Created] val data: 44723\n",
      "[Dataset ID Dict Created] test data: 114412\n"
     ]
    }
   ],
   "source": [
    "data_folder = \"\"\n",
    "data_chunk_folder = \"ecg-pat40-tch-sinus_jet_lead2\"\n",
    "load_training_data = False\n",
    "\n",
    "patient_ID_list_train = [398573, 462229, 637891, 667681, 537854, 628521, 642321, 662493,\n",
    "                         387479, 624179, 417349, 551554, 631270, 655769, 678877]  # 15\n",
    "patient_ID_list_test = [756172, 424072, 748555, 748900, 759678, 741235, 595561, 678607,\n",
    "                        782501, 510915, 771495, 740475, 533362, 581650, 803389, 577874,\n",
    "                        681150, 536886, 477589, 844864, 824744, 515544, 771958, 725860, 609090]  # 25\n",
    "patient_ID_list_val = [462229, 642321, 387479]  # 3\n",
    "patient_ID_list_dev = [patient_ID for patient_ID in patient_ID_list_train if\n",
    "                       patient_ID not in patient_ID_list_val]  # 12\n",
    "\n",
    "if load_training_data:\n",
    "    data_chunk_list = []\n",
    "    for data_filename in os.listdir(os.path.join(data_folder, data_chunk_folder)):\n",
    "        data_chunk_list.append(pd.read_csv(os.path.join(data_folder, data_chunk_folder, data_filename)))\n",
    "    feature_df_all_selected_with_ecg = pd.concat(data_chunk_list, axis=0)\n",
    "    channel_ID = 2\n",
    "    feature_with_ecg_df_dev_single_lead = feature_df_all_selected_with_ecg.query(f\"patient_ID in {patient_ID_list_dev}\").query(f\"channel_ID == {channel_ID}\") \n",
    "    print(f\"[Dataset Loaded] dev data: {feature_with_ecg_df_dev_single_lead.shape}\")\n",
    "else:\n",
    "    feature_with_ecg_df_dev_single_lead = pd.DataFrame()\n",
    "    dev_dataset = None\n",
    "\n",
    "feature_with_ecg_df_val_single_lead = pd.read_csv(\"feature_with_ecg_df_val_lead2.csv\")\n",
    "feature_with_ecg_df_test_single_lead = pd.read_csv(\"feature_with_ecg_df_test_lead2.csv\")\n",
    "print(f\"[Dataset Loaded] val data: {feature_with_ecg_df_val_single_lead.shape}\")\n",
    "print(f\"[Dataset Loaded] test data: {feature_with_ecg_df_test_single_lead.shape}\")\n",
    "if load_training_data:\n",
    "    dev_dataset = ECG_classification_dataset_with_peak_features(feature_with_ecg_df_dev_single_lead,\n",
    "                                                                 shift_signal=False,\n",
    "                                                                 shift_amount=0,\n",
    "                                                                 normalize_signal=True,\n",
    "                                                                 ecg_resampling_length_target=300,\n",
    "                                                                 return_id_vec=True)\n",
    "    print(f\"[Dataset Created] dev data: {len(dev_dataset)}\")\n",
    "    \n",
    "val_dataset = ECG_classification_dataset_with_peak_features(feature_with_ecg_df_val_single_lead,\n",
    "                                                                     shift_signal=False,\n",
    "                                                                     shift_amount=0,\n",
    "                                                                     normalize_signal=True,\n",
    "                                                                     ecg_resampling_length_target=300,\n",
    "                                                                     return_id_vec=True)\n",
    "print(f\"[Dataset Created] val data: {len(val_dataset)}\")\n",
    "\n",
    "test_dataset = ECG_classification_dataset_with_peak_features(feature_with_ecg_df_test_single_lead,\n",
    "                                                                     shift_signal=False,\n",
    "                                                                     shift_amount=0,\n",
    "                                                                     normalize_signal=True,\n",
    "                                                                     ecg_resampling_length_target=300,\n",
    "                                                                     return_id_vec=True)\n",
    "print(f\"[Dataset Created] test data: {len(test_dataset)}\")\n",
    "    \n",
    "if load_training_data:\n",
    "    dataset_id_dict_dev = Get_dataset_id_dict(dev_dataset)\n",
    "    print(f\"[Dataset ID Dict Created] dev data: {len(dataset_id_dict_dev)}\")\n",
    "else:\n",
    "    dataset_id_dict_dev = None\n",
    "    \n",
    "dataset_id_dict_val = Get_dataset_id_dict(val_dataset)\n",
    "print(f\"[Dataset ID Dict Created] val data: {len(dataset_id_dict_val)}\")\n",
    "dataset_id_dict_test = Get_dataset_id_dict(test_dataset)\n",
    "print(f\"[Dataset ID Dict Created] test data: {len(dataset_id_dict_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632ad42",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eef1f24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:11:06.209499Z",
     "start_time": "2023-10-03T16:11:05.946500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt_path: trained_models\\linear\\s6qzuo6i\\supervised-clocsCNN_1D-ECG_normalized-20221202_v30-maxep200-early_stopping_patience40-mixup-label_smoothing-aug_selected_20221029_prob0__2-s6qzuo6i-epoch=162-val_auroc=0.9495.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pretrained_feature_extractor': None,\n",
       " 'validation_frequency': 1,\n",
       " 'dataset': 'ecg-TCH-40_patient-20220201',\n",
       " 'data_dir': None,\n",
       " 'train_dir': None,\n",
       " 'val_dir': None,\n",
       " 'dali': False,\n",
       " 'dali_device': 'gpu',\n",
       " 'no_labels': True,\n",
       " 'debug': False,\n",
       " 'seed': 0,\n",
       " 'ptl_accelerator': 'ddp',\n",
       " 'cluster_name': 'auto',\n",
       " 'read_data_by_chunk': True,\n",
       " 'data_chunk_folder': 'ecg-pat40-tch-sinus_jet',\n",
       " 'channel_ID': 2,\n",
       " 'shift_signal': False,\n",
       " 'normalize_signal': True,\n",
       " 'ecg_resampling_length': 300,\n",
       " 'ecg_resampling_length_target': 300,\n",
       " 'stride': 2,\n",
       " 'c4_multiplier': 3,\n",
       " 'n_classes': 2,\n",
       " 'n_length': 300,\n",
       " 'num_layers': 3,\n",
       " 'd_model': 1,\n",
       " 'nhead': 1,\n",
       " 'dim_feedforward': 128,\n",
       " 'dropout': 0.1,\n",
       " 'activation': 'relu',\n",
       " 'use_raw_patch': False,\n",
       " 'transforms': 'SelectedAug_20221029',\n",
       " 'wandb_dir': '/mnt/data/group1/yilong/JET-Detection-Data/adios',\n",
       " 'save_eval_dataset': False,\n",
       " 'aug_prob': 1.25,\n",
       " 'mixup_alpha': 0.2,\n",
       " 'label_smoothing': 0.2,\n",
       " 'patience': 40,\n",
       " 'in_channels': 1,\n",
       " 'in_channels_type': 'ECG',\n",
       " 'logger': True,\n",
       " 'checkpoint_callback': True,\n",
       " 'default_root_dir': None,\n",
       " 'gradient_clip_val': 0.0,\n",
       " 'gradient_clip_algorithm': 'norm',\n",
       " 'process_position': 0,\n",
       " 'num_nodes': 1,\n",
       " 'num_processes': 1,\n",
       " 'devices': None,\n",
       " 'gpus': [0],\n",
       " 'auto_select_gpus': False,\n",
       " 'tpu_cores': None,\n",
       " 'ipus': None,\n",
       " 'log_gpu_memory': None,\n",
       " 'progress_bar_refresh_rate': None,\n",
       " 'overfit_batches': 0.0,\n",
       " 'track_grad_norm': -1,\n",
       " 'check_val_every_n_epoch': 1,\n",
       " 'fast_dev_run': False,\n",
       " 'accumulate_grad_batches': 1,\n",
       " 'max_epochs': 200,\n",
       " 'min_epochs': None,\n",
       " 'max_steps': None,\n",
       " 'min_steps': None,\n",
       " 'max_time': None,\n",
       " 'limit_train_batches': 1.0,\n",
       " 'limit_val_batches': 1.0,\n",
       " 'limit_test_batches': 1.0,\n",
       " 'limit_predict_batches': 1.0,\n",
       " 'val_check_interval': 1.0,\n",
       " 'flush_logs_every_n_steps': 100,\n",
       " 'log_every_n_steps': 50,\n",
       " 'accelerator': None,\n",
       " 'sync_batchnorm': True,\n",
       " 'precision': 32,\n",
       " 'weights_summary': 'top',\n",
       " 'weights_save_path': None,\n",
       " 'num_sanity_val_steps': 2,\n",
       " 'truncated_bptt_steps': None,\n",
       " 'resume_from_checkpoint': None,\n",
       " 'profiler': None,\n",
       " 'benchmark': False,\n",
       " 'deterministic': False,\n",
       " 'reload_dataloaders_every_n_epochs': 0,\n",
       " 'reload_dataloaders_every_epoch': False,\n",
       " 'auto_lr_find': False,\n",
       " 'replace_sampler_ddp': True,\n",
       " 'terminate_on_nan': False,\n",
       " 'auto_scale_batch_size': False,\n",
       " 'prepare_data_per_node': True,\n",
       " 'plugins': None,\n",
       " 'amp_backend': 'native',\n",
       " 'amp_level': 'O2',\n",
       " 'distributed_backend': None,\n",
       " 'move_metrics_to_cpu': False,\n",
       " 'multiple_trainloader_mode': 'max_size_cycle',\n",
       " 'stochastic_weight_avg': False,\n",
       " 'encoder': 'clocs_cnn1d',\n",
       " 'zero_init_residual': None,\n",
       " 'batch_size': 128,\n",
       " 'lr': 0.005,\n",
       " 'classifier_lr': 0.001,\n",
       " 'weight_decay': 1e-06,\n",
       " 'num_workers': 4,\n",
       " 'name': 'supervised-clocsCNN_1D-ECG_normalized-20221202_v30-maxep200-early_stopping_patience40-mixup-label_smoothing-aug_selected_20221029_prob0__2',\n",
       " 'project': None,\n",
       " 'entity': None,\n",
       " 'wandb': True,\n",
       " 'offline': False,\n",
       " 'optimizer': 'adam',\n",
       " 'lars': True,\n",
       " 'exclude_bias_n_norm': True,\n",
       " 'scheduler': 'warmup_cosine',\n",
       " 'lr_decay_steps': None,\n",
       " 'use_mask': False,\n",
       " 'train_backbone': True,\n",
       " 'mask_feature_extractor': None,\n",
       " 'embedding_dim': 256,\n",
       " 'ckpt_epoch': -1,\n",
       " 'checkpoint_dir': '/mnt/data/group1/yilong/JET-Detection-Data/adios/trained_models',\n",
       " 'checkpoint_frequency': 1,\n",
       " 'auto_mask_dir': '/mnt/data/group1/yilong/JET-Detection-Data/adios/auto_mask',\n",
       " 'target_type': 'single',\n",
       " 'cifar': False,\n",
       " 'extra_optimizer_args': {},\n",
       " 'num_examples': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict = {}\n",
    "model_ID_dict = {}\n",
    "model_ID = \"s6qzuo6i\"\n",
    "ckpt_dir = Path(f\"trained_models/linear/{model_ID}\") # test_auroc = 0.9631\n",
    "max_val_auroc = max([float(ele.split(\"=\")[-1].replace(\".ckpt\", \"\")) \\\n",
    "                     for ele in os.listdir(ckpt_dir) if \"=\" in ele and \"val_auroc\" in ele])\n",
    "ckpt_path = [ckpt_dir / ckpt for ckpt in os.listdir(ckpt_dir) if ckpt.endswith(f\"val_auroc={max_val_auroc}.ckpt\")][0]\n",
    "args_path = ckpt_dir / \"args.json\"\n",
    "print(f\"ckpt_path: {ckpt_path}\")\n",
    "with open(args_path) as f:\n",
    "    method_args_default = json.load(f)\n",
    "method_args_default[\"num_examples\"] = 1\n",
    "method_args_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b85184f",
   "metadata": {},
   "source": [
    "## Load a matrix of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601249e4",
   "metadata": {},
   "source": [
    "### ClocsCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cee5c61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:11:13.138499Z",
     "start_time": "2023-10-03T16:11:12.688501Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_ID = cqpywv9j\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n",
      "model_ID = fbmrv1zn\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n",
      "model_ID = g09je3xq\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n",
      "model_ID = jmc8vlus\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n",
      "model_ID = ppmvi63y\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n",
      "model_ID = s565epz8\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n",
      "model_ID = s6qzuo6i\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n",
      "model_ID = sbqkv8ud\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\jpy3.10\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\jpy3.10\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_ID = t2ykqoga\n",
      "2 0.1 0.1 0.1 1 256 CLOCS \n",
      "Using kernel size: 7\n",
      "c1, c2, k, s, c3, c4, c4_multiplier, embedding_dim: (1, 4, 7, 2, 16, 32, 3, 256)\n",
      "classifier feat_in = 256\n",
      "self.extra_optimizer_args: {}\n",
      "self.loss_type: ce, self.sat_momentum: 0.9, self.sat_pretrain: 60\n"
     ]
    }
   ],
   "source": [
    "model_folder_name = \"supervised-clocsCNN_1D-ECG_normalized-20230220_v30-deep_ensemble-maxep200-early_stopping_patience40-mixup-label_smoothing-aug_selected_20221029-prob1.25_2\"\n",
    "model_folder_path = Path(f\"trained_models/linear/{model_folder_name}\")\n",
    "model_ID_list = os.listdir(os.path.join(model_folder_path))\n",
    "for i, model_ID in enumerate(model_ID_list):\n",
    "#     model_ID = \"s6qzuo6i\"\n",
    "    print(f\"model_ID = {model_ID}\")\n",
    "    model_key = f\"ClocsCNN_ens_{i}\"\n",
    "    model_ID_dict[model_key] = model_ID\n",
    "    embedding_dim = 256\n",
    "    model = cnn_network_contrastive(zero_init_residual=None, embedding_dim=embedding_dim,\n",
    "                       stride=2, c4_multiplier=3)\n",
    "    model.fc = nn.Identity()\n",
    "    ckpt_dir = Path(f\"trained_models/linear/{model_folder_name}/{model_ID}\") # test_auroc = 0.9631\n",
    "    max_val_auroc = max([float(ele.split(\"=\")[-1].replace(\".ckpt\", \"\")) \\\n",
    "                         for ele in os.listdir(ckpt_dir) if \"=\" in ele and \"val_auroc\" in ele])\n",
    "    ckpt_path = [ckpt_dir / ckpt for ckpt in os.listdir(ckpt_dir) if ckpt.endswith(f\"val_auroc={max_val_auroc:.4f}.ckpt\")][0]\n",
    "    method_args = method_args_default\n",
    "    model.pretrained_occlusion_model_dict = None\n",
    "    method_args[\"backbone\"] = model\n",
    "    model_loaded = SupervisedModel_1D.load_from_checkpoint(\n",
    "        ckpt_path, strict=False, **method_args\n",
    "    )\n",
    "    model_dict[model_key] = model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07a789b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:11:19.555501Z",
     "start_time": "2023-10-03T16:11:19.305500Z"
    }
   },
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\n",
    "# print(checkpoint[\"hyper_parameters\"])\n",
    "# print(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08316902",
   "metadata": {},
   "source": [
    "# Get softmax scores from models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64efc584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:11:25.774530Z",
     "start_time": "2023-10-03T16:11:25.524499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ClocsCNN_ens_0', 'ClocsCNN_ens_1', 'ClocsCNN_ens_2', 'ClocsCNN_ens_3', 'ClocsCNN_ens_4', 'ClocsCNN_ens_5', 'ClocsCNN_ens_6', 'ClocsCNN_ens_7', 'ClocsCNN_ens_8'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a615b904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:11:32.296501Z",
     "start_time": "2023-10-03T16:11:32.021499Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataloader Created] val data: 1398\n",
      "[Dataloader Created] test data: 3576\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_workers = 0\n",
    "pin_memory = False\n",
    "if load_training_data:\n",
    "    dev_loader = DataLoader(\n",
    "        dev_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, pin_memory=pin_memory\n",
    "    )\n",
    "    print(f\"[Dataloader Created] dev data: {len(dev_loader)}\")\n",
    "else:\n",
    "    dev_loader = None\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, pin_memory=pin_memory\n",
    ")\n",
    "print(f\"[Dataloader Created] val data: {len(val_loader)}\")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False, drop_last=False, pin_memory=pin_memory\n",
    ")\n",
    "print(f\"[Dataloader Created] test data: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "173f6c4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:11:38.459500Z",
     "start_time": "2023-10-03T16:11:38.212503Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    \"dev\": feature_with_ecg_df_dev_single_lead,\n",
    "    \"val\": feature_with_ecg_df_val_single_lead,\n",
    "    \"test\": feature_with_ecg_df_test_single_lead\n",
    "}\n",
    "dataset_dict = {\n",
    "    \"dev\": dev_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n",
    "dataset_id_dict_dict = {\n",
    "    \"dev\": dataset_id_dict_dev,\n",
    "    \"val\": dataset_id_dict_val,\n",
    "    \"test\": dataset_id_dict_test\n",
    "}\n",
    "data_loader_dict = {\n",
    "    \"dev\": dev_loader,\n",
    "    \"val\": val_loader,\n",
    "    \"test\": test_loader\n",
    "}\n",
    "roc_curve_results_dict_dict_dict = {\"dev\": {}, \"val\": {}, \"test\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf7f886a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:11:50.554499Z",
     "start_time": "2023-10-03T16:11:50.276504Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)\n",
    "torch.get_num_threads()\n",
    "\n",
    "roc_curve_results_dict_dict_dict_folder_name = f\"roc_curve_results_dict_dict_dict_folder\"\n",
    "os.makedirs(f\"roc_curve_results_dict_dict_dict_folder\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3ce05a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:12:03.095529Z",
     "start_time": "2023-10-03T16:12:02.717503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time 0.0] val ClocsCNN_ens_0 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.0] val ClocsCNN_ens_1 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.0] val ClocsCNN_ens_2 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.0] val ClocsCNN_ens_3 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.0] val ClocsCNN_ens_4 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.0] val ClocsCNN_ens_5 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.0] val ClocsCNN_ens_6 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.0] val ClocsCNN_ens_7 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] val ClocsCNN_ens_8 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_0 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_1 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_2 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_3 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_4 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_5 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_6 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_7 use_gpu = True\n",
      "Loaded.\n",
      "[Time 0.1] test ClocsCNN_ens_8 use_gpu = True\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "target_fpr = 0.05\n",
    "st = time.time()\n",
    "for mode in [\"val\", \"test\"]:\n",
    "    for model_key in model_dict:\n",
    "        use_gpu = True\n",
    "#         use_gpu = False\n",
    "        model_ID = model_ID_dict[model_key]\n",
    "        save_name = f\"roc_curve_results_dict_dict_dict-{model_ID}-{mode}.pickle\"\n",
    "        save_path = os.path.join(roc_curve_results_dict_dict_dict_folder_name, save_name)\n",
    "        print(f\"[Time {time.time() - st:.1f}]\", mode, model_key, f\"use_gpu = {use_gpu}\")\n",
    "        if model_key not in roc_curve_results_dict_dict_dict[mode]:\n",
    "            if os.path.exists(save_path):\n",
    "                with open(save_path, \"rb\") as f:\n",
    "                    roc_curve_results_dict_dict_tmp = pickle.load(f)\n",
    "                roc_curve_results_dict_dict_dict[mode][model_key] = roc_curve_results_dict_dict_tmp\n",
    "                print(f\"Loaded.\")\n",
    "            else:\n",
    "                roc_curve_results_dict_dict_dict[mode][model_key] = Get_roc_curve_df_from_model_and_data_loader(\n",
    "                    model_dict[model_key], data_loader_dict[mode], target_fpr=target_fpr, target_threshold=None, abstain=True)\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    pickle.dump(roc_curve_results_dict_dict_dict[mode][model_key], f)\n",
    "                print(f\"Saved.\")\n",
    "#             torch.cuda.empty_cache()\n",
    "        else:\n",
    "            print(f\"Skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34d89059",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T17:34:51.545031Z",
     "start_time": "2023-10-03T17:34:51.307033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2237, 0.2715, 0.2414, 0.2427, 0.4258, 0.1931, 0.2766, 0.1171, 0.2294,\n",
       "        0.2520])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_curve_results_dict_dict_dict[\"val\"][\"ClocsCNN_ens_0\"][\"y_scores\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5903713f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:12:15.225501Z",
     "start_time": "2023-10-03T16:12:14.943501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mode</th>\n",
       "      <th>model_key</th>\n",
       "      <th>auroc</th>\n",
       "      <th>tpr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_0</td>\n",
       "      <td>0.950550</td>\n",
       "      <td>0.888298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_6</td>\n",
       "      <td>0.949504</td>\n",
       "      <td>0.869930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_2</td>\n",
       "      <td>0.950173</td>\n",
       "      <td>0.865175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_3</td>\n",
       "      <td>0.946743</td>\n",
       "      <td>0.863963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_7</td>\n",
       "      <td>0.916976</td>\n",
       "      <td>0.860606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_4</td>\n",
       "      <td>0.938610</td>\n",
       "      <td>0.857529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_5</td>\n",
       "      <td>0.944179</td>\n",
       "      <td>0.855198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_8</td>\n",
       "      <td>0.894858</td>\n",
       "      <td>0.769604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>val</td>\n",
       "      <td>ClocsCNN_ens_1</td>\n",
       "      <td>0.896154</td>\n",
       "      <td>0.680186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_6</td>\n",
       "      <td>0.950648</td>\n",
       "      <td>0.828230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_1</td>\n",
       "      <td>0.938126</td>\n",
       "      <td>0.815158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_2</td>\n",
       "      <td>0.951888</td>\n",
       "      <td>0.806189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_0</td>\n",
       "      <td>0.939464</td>\n",
       "      <td>0.776902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_5</td>\n",
       "      <td>0.931451</td>\n",
       "      <td>0.761583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_8</td>\n",
       "      <td>0.933123</td>\n",
       "      <td>0.727757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_3</td>\n",
       "      <td>0.935651</td>\n",
       "      <td>0.694979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_7</td>\n",
       "      <td>0.936048</td>\n",
       "      <td>0.694738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test</td>\n",
       "      <td>ClocsCNN_ens_4</td>\n",
       "      <td>0.932875</td>\n",
       "      <td>0.658861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mode       model_key     auroc       tpr\n",
       "0    val  ClocsCNN_ens_0  0.950550  0.888298\n",
       "6    val  ClocsCNN_ens_6  0.949504  0.869930\n",
       "2    val  ClocsCNN_ens_2  0.950173  0.865175\n",
       "3    val  ClocsCNN_ens_3  0.946743  0.863963\n",
       "7    val  ClocsCNN_ens_7  0.916976  0.860606\n",
       "4    val  ClocsCNN_ens_4  0.938610  0.857529\n",
       "5    val  ClocsCNN_ens_5  0.944179  0.855198\n",
       "8    val  ClocsCNN_ens_8  0.894858  0.769604\n",
       "1    val  ClocsCNN_ens_1  0.896154  0.680186\n",
       "15  test  ClocsCNN_ens_6  0.950648  0.828230\n",
       "10  test  ClocsCNN_ens_1  0.938126  0.815158\n",
       "11  test  ClocsCNN_ens_2  0.951888  0.806189\n",
       "9   test  ClocsCNN_ens_0  0.939464  0.776902\n",
       "14  test  ClocsCNN_ens_5  0.931451  0.761583\n",
       "17  test  ClocsCNN_ens_8  0.933123  0.727757\n",
       "12  test  ClocsCNN_ens_3  0.935651  0.694979\n",
       "16  test  ClocsCNN_ens_7  0.936048  0.694738\n",
       "13  test  ClocsCNN_ens_4  0.932875  0.658861"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_curve_results_df_dict = dict.fromkeys([\"mode\", \"model_key\", \"auroc\", \"tpr\"])\n",
    "for k in roc_curve_results_df_dict:\n",
    "    roc_curve_results_df_dict[k] = []\n",
    "    \n",
    "for mode in [\"val\", \"test\"]:\n",
    "    for model_key in model_dict:\n",
    "        roc_curve_results_df_dict[\"mode\"].append(mode)\n",
    "        roc_curve_results_df_dict[\"model_key\"].append(model_key)\n",
    "        roc_curve_results_df_dict[\"auroc\"].append(roc_curve_results_dict_dict_dict[mode][model_key][\"auroc\"].item())\n",
    "        roc_curve_df = roc_curve_results_dict_dict_dict[mode][model_key][\"roc_curve_df\"]\n",
    "        roc_curve_df_target_fpr = roc_curve_df.iloc[(roc_curve_df[\"fpr\"] - target_fpr).abs().argsort()[0]]\n",
    "        resulting_tpr = roc_curve_df_target_fpr[\"tpr\"]\n",
    "#         print(f\"mode: {mode}, model: {model_key}, len(roc_curve_df): {len(roc_curve_df)}\")\n",
    "        roc_curve_results_df_dict[\"tpr\"].append(resulting_tpr)\n",
    " \n",
    "roc_curve_results_df = pd.DataFrame.from_dict(roc_curve_results_df_dict).sort_values(by=[\"mode\", \"tpr\"], ascending=False)\n",
    "roc_curve_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6444509",
   "metadata": {},
   "source": [
    "# Covert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17871b5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:12:26.816498Z",
     "start_time": "2023-10-03T16:12:26.595501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Dropbox\\\\Study\\\\GitHub\\\\JET-Detection\\\\[20230920] JET Detection Deployment Code\\\\Model Checkpoints'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_folder = os.path.normpath(\"D:\\Dropbox\\Study\\GitHub\\JET-Detection\\[20230920] JET Detection Deployment Code\\Model Checkpoints\")\n",
    "checkpoint_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6806f656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:12:38.156501Z",
     "start_time": "2023-10-03T16:12:37.939501Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name_dict = {\n",
    "    \"ClocsCNN_ens_0\": \"ECG_DL_MODEL_1\",\n",
    "    \"ClocsCNN_ens_6\": \"ECG_DL_MODEL_2\",\n",
    "    \"ClocsCNN_ens_2\": \"ECG_DL_MODEL_3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06971552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:12:49.578503Z",
     "start_time": "2023-10-03T16:12:49.361523Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model_to_convert(nn.Module):\n",
    "    def __init__(self, backbone, classifier):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).cpu()\n",
    "        x = self.classifier(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5db853a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T16:18:29.664612Z",
     "start_time": "2023-10-03T16:18:28.998104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Diagnostic Run torch.onnx.export version 2.1.0.dev20230508+cu121 =======\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "======= Diagnostic Run torch.onnx.export version 2.1.0.dev20230508+cu121 =======\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "======= Diagnostic Run torch.onnx.export version 2.1.0.dev20230508+cu121 =======\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n"
     ]
    }
   ],
   "source": [
    "selected_model_name_list = [\"ClocsCNN_ens_0\", \"ClocsCNN_ens_6\", \"ClocsCNN_ens_2\"]\n",
    "\n",
    "batch_size = 1\n",
    "for selected_model_name in selected_model_name_list:\n",
    "#     selected_model_name = selected_model_name_list[0]\n",
    "    model_loaded = model_dict[selected_model_name].cpu()\n",
    "    model_loaded.eval()\n",
    "    backbone = model_loaded.backbone.cpu()\n",
    "    classifier = model_loaded.classifier.cpu()\n",
    "    torch_model = Model_to_convert(backbone, classifier)\n",
    "    torch_model.eval()\n",
    "    x = torch.randn(batch_size, 1, 300, requires_grad=True)\n",
    "    torch_out = torch_model(x)\n",
    "\n",
    "    # Export the model\n",
    "    torch.onnx.export(torch_model,               # model being run\n",
    "                      x,                         # model input (or a tuple for multiple inputs)\n",
    "                      os.path.join(checkpoint_folder, f\"{model_name_dict[selected_model_name]}.onnx\"),   # where to save the model (can be a file or file-like object)\n",
    "                      verbose=True,\n",
    "                      export_params=True,        # store the trained parameter weights inside the model file\n",
    "                      opset_version=11,          # the ONNX version to export the model to\n",
    "                      do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                      input_names = ['input'],   # the model's input names\n",
    "                      output_names = ['output'], # the model's output names\n",
    "                      dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
    "                                    'output' : {0 : 'batch_size'}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "550.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
